{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4SAGeXapyM4"
      },
      "source": [
        "# LoRA Fine-tuning\n",
        "\n",
        "In this example, we'll fine-tune a base ViT model with two different datasets using LoRA. Then, we'll load the base model and dynamically swap both LoRA adapters depending on the task we want to complete.\n",
        "\n",
        "## Why does this matter?\n",
        "\n",
        "A foundation model knows how to do many things, but it's not great at many tasks. We can fine-tune the model to produce specialized models that are very good at solving specific tasks.\n",
        "\n",
        "We'll use LoRA to fine-tune the foundation model and generate many, specialized adapters. We can load these adapters together with a model to dynamically transform its capabilities.\n",
        "\n",
        "When loading the model, we'll take the foundation model's original weights and apply the LoRA weight changes to it to get the fine-tuned model weights.\n",
        "\n",
        "The beauty of LoRA is that we don't need to fine-tune the entire matrix of weights. Instead, we can get away by fine-tuning two matrices of lower rank. These matrices, when multiplied together, will get us the weight updates we'll need to apply the foundation model to modify its capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are the **steps** we will follow through this tutorial to go from raw image datasets all the way to deploying lightweight, LoRA-fine-tuned Vision Transformer adapters for inference:\n",
        "- 1. **Setup & Utilities**: Install necessary libraries, define helper functions for model sizing, parameter reporting, dataset splitting, and label-ID mappings.\n",
        "- 2. **Data Preparation**: Load and preprocess the Food101 and Cats vs Dogs datasets, split into train/test, and build label2id/id2label mappings so the model knows how to translate between strings and indices.\n",
        "- 3. **Fine-Tuning the Model : Model & LoRA Configuration**: Load the pretrained ViT base model, wrap it with a LoRA adapter configuration (specifying low-rank update matrices), and inspect trainable vs. total parameters.\n",
        "- 4. **Fine-Tuning Loop**: Use a single Trainer loop that, for each dataset configuration, applies our TrainingArguments, trains the LoRA-augmented model, evaluates validation accuracy each epoch, and saves the best adapter.\n",
        "- 5. **Deployment & Running Inference**: Load each saved LoRA adapter on top of the base ViT, retrieve the matching image processor, and run new images through the predict function to obtain human-readable class labels."
      ],
      "metadata": {
        "id": "eQtuHwn3Vy5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup & Utilities\n",
        "\n",
        "We will download the following packages :\n",
        "- `transformers` : a library of state-of-the-art pretrained models (BERT, GPT, T5, Vision Transformers, etc.) and high-level pipelines. We use it to load or define models for fine-tuning or inference. To learn more about it, click [here](https://huggingface.co/docs/transformers/en/quicktour).\n",
        "- `datasets` : a fast, memory-mapped library for loading and processing datasets at scale. We use it to download, preprocess (e.g. tokenization), and batch data for training or evaluation.To learn more about it, click [here](https://huggingface.co/docs/datasets/en/installation).\n",
        "- `evaluate` : a lightweight toolkit for computing evaluation metrics like accuracy, F1, BLEU, ROUGE, perplexity. It is very easy to integrate with the HuggingFace Trainer or custom loops. We use it to compute validation/test metrics to monitor performance.\n",
        "- `accelerate` : a thin wrapper to simplify multi-GPU/multi-node training and mixed-precision. It allows us to launch our model in one line `accelerate launch train.py`. We use it to scale training from a single GPU to many GPUs with minimal code changes.\n",
        "- `peft` : stands for Parameter-Efficient Fine-Tuning. It is a techniques for fine-tuning large models using only a small number of additional parameters. LoRA is one of the core PEFT methods. We use this library to adapt huge models on your data without full fine-tuning, reducing memory & compute costs.\n"
      ],
      "metadata": {
        "id": "w9CIt2LrV6Vu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adotjRk7pyM5",
        "outputId": "051fb2ec-7c0a-465d-961b-b18593bb724f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install --quiet transformers accelerate evaluate datasets peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE-F04x1pyM5"
      },
      "source": [
        "We are going to use a **Vision Transformer (ViT) model** pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. ViT-Base-Patch16-224 is the â€œbaseâ€ variant of the Vision Transformer that divides a 224Ã—224 input image into non-overlapping 16Ã—16 patches, projects each patch into a 768-dimensional embedding, and processes the resulting 14Ã—14 sequence with a 12-layer transformer. It was pretrained on ImageNet-1k to show that pure self-attention architectures can match or outperform convolutional models on image classification. You can learn more about it in the [model card](https://huggingface.co/google/vit-base-patch16-224).\n",
        "This model has a size of 346 MB on disk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1k8FV6ppyM6"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJhXxhD8pyM6"
      },
      "source": [
        "### Creating A Couple Of Helpful Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make our Python notebook cleaner, we will create what we call â€œhelper functionsâ€ to help us abstract away repetitive tasksâ€”such as :\n",
        "- measuring model size with `print_model_size(path)`\n",
        "- reporting trainable parameters with `print_trainable_parameters(model, label)`\n",
        "- splitting datasets with `split_dataset(dataset)`\n",
        "- generating label mappings with `create_label_mappings(dataset)`\n",
        "So that our main training and evaluation code stays concise and easy to read.\n"
      ],
      "metadata": {
        "id": "eFp73j7VWfcd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh2WResVpyM6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "\n",
        "def print_model_size(path):\n",
        "    size = 0\n",
        "    for f in os.scandir(path):\n",
        "        size += os.path.getsize(f)\n",
        "\n",
        "    print(f\"Model size: {(size / 1e6):.2} MB\")\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model, label):\n",
        "    parameters, trainable = 0, 0\n",
        "\n",
        "    for _, p in model.named_parameters():\n",
        "        parameters += p.numel()\n",
        "        trainable += p.numel() if p.requires_grad else 0\n",
        "\n",
        "    print(f\"{label} trainable parameters: {trainable:,}/{parameters:,} ({100 * trainable / parameters:.2f}%)\")\n",
        "\n",
        "\n",
        "def split_dataset(dataset):\n",
        "    dataset_splits = dataset.train_test_split(test_size=0.1)\n",
        "    return dataset_splits.values()\n",
        "\n",
        "\n",
        "def create_label_mappings(dataset):\n",
        "    label2id, id2label = dict(), dict()\n",
        "    for i, label in enumerate(dataset.features[\"label\"].names):\n",
        "        label2id[label] = i\n",
        "        id2label[i] = label\n",
        "\n",
        "    return label2id, id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovf43z7mpyM7"
      },
      "source": [
        "## Loading and Preparing the Datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll be loading two different datasets to fine-tune the base model using the `datasets` package:\n",
        "- A dataset of pictures of food `food101`, more info [here](\n",
        "- A dataset of pictures of cats and dogs `microsoft/cats_vs_dogs`, more info [here](\n",
        "\n",
        "Before running the following code, you need to have your HuggingFace token saved in secret key. If you dont, here's how to create one :\n",
        "Google Colab has a built-in â€œSecretsâ€ pane that lets you store API keys (or any other secret) so they never get hard-coded into your notebook. Hereâ€™s how to add your Hugging Face token and consume it in your code.\n",
        "\n",
        "To add your token to Colabâ€™s Secrets\n",
        "1. Open your Colab notebook.  \n",
        "2. Click the **ğŸ”‘ Secrets** icon on the left sidebar.  \n",
        "3. In the Secrets panel that appears, click **+ Add a secret**.  \n",
        "4. In the **Key** field enter \"HUGGINGFACE_TOKEN\"\n",
        "5. In the **Value** field paste your Hugging Face token (the string you copied from your Hugging Face profile, if you dont have you can create a new one following this [tutorial](https://www.geeksforgeeks.org/how-to-access-huggingface-api-key/)).  \n",
        "6. Click **Save**.  \n",
        "7. Close the Secrets pane.\n"
      ],
      "metadata": {
        "id": "sW-mWHYZWlNf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeEZ6rLSpyM7"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# This is the food dataset\n",
        "dataset1 = load_dataset(\"food101\", split=\"train[:10000]\") # tells load_dataset to load only the first 10,000 examples from the â€œtrainâ€ split of the Food101 dataset\n",
        "\n",
        "# This is the datasets of pictures of cats and dogs.\n",
        "# Notice we need to rename the label column so we can\n",
        "# reuse the same code for both datasets.\n",
        "dataset2 = load_dataset(\"microsoft/cats_vs_dogs\", split=\"train\", trust_remote_code=True)\n",
        "dataset2 = dataset2.rename_column(\"labels\", \"label\")\n",
        "\n",
        "dataset1_train, dataset1_test = split_dataset(dataset1)\n",
        "dataset2_train, dataset2_test = split_dataset(dataset2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWi26oF-pyM7"
      },
      "source": [
        "We need these mappings to properly fine-tune the Vision Transformer model. You can find more information in the [`PretrainedConfig`](https://huggingface.co/docs/transformers/en/main_classes/configuration#transformers.PretrainedConfig) documentation, under the \"Parameters for fine-tuning tasks\" section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4S9KwlppyM8"
      },
      "outputs": [],
      "source": [
        "dataset1_label2id, dataset1_id2label = create_label_mappings(dataset1)\n",
        "dataset2_label2id, dataset2_id2label = create_label_mappings(dataset2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You then pass these into your modelâ€™s configuration (via PretrainedConfig) so that:\n",
        "- During training, when you see a sample labeled \"dog\", you can look up label2id[\"dog\"] â†’ 1 and compute the loss against class index 1.\n",
        "- During evaluation or inference, the model will predict a class index (say, 2) and you can convert that back to the humanâ€readable label \"elephant\" via id2label[2].\n",
        "\n",
        "Without these mappings, the model wouldnâ€™t know how to translate between the string labels in your data and the integer IDs it actually trains on."
      ],
      "metadata": {
        "id": "tvz8PamhWvnX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHKdprWopyM8"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"model1\": {\n",
        "        \"train_data\": dataset1_train,\n",
        "        \"test_data\": dataset1_test,\n",
        "        \"label2id\": dataset1_label2id,\n",
        "        \"id2label\": dataset1_id2label,\n",
        "        \"epochs\": 5,\n",
        "        \"path\": \"./lora-model1\"\n",
        "    },\n",
        "    \"model2\": {\n",
        "        \"train_data\": dataset2_train,\n",
        "        \"test_data\": dataset2_test,\n",
        "        \"label2id\": dataset2_label2id,\n",
        "        \"id2label\": dataset2_id2label,\n",
        "        \"epochs\": 1,\n",
        "        \"path\": \"./lora-model2\"\n",
        "    },\n",
        "}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGdS25aKpyM8"
      },
      "source": [
        "config is simply a Python dictionary that acts as a centralized registry of all the settings and data you need for each of your training runs. Rather than hard-coding values all over your script, you bundle them together under human-readable keys (here, \"model1\" and \"model2\"). Each entry contains:\n",
        "- train_data / test_data: the dataset splits youâ€™ll use to teach and evaluate the model\n",
        "- label2id / id2label: the mappings between your string class names and the integer IDs the model actually learns\n",
        "- epochs: how many full passes over the training set you want for that particular experiment\n",
        "- path: the filesystem location where youâ€™ll save the fine-tuned (LoRA-augmented) model\n",
        "\n",
        "By organizing everything into one config dict, you can write a single loop that:\n",
        "- Picks up the right datasets and mappings for each model\n",
        "- Reads the correct number of epochs\n",
        "- Saves each experimentâ€™s outputs in its own folder\n",
        "\n",
        "This pattern keeps your code DRY (Donâ€™t Repeat Yourself), makes it easy to add new experiments (just add another key to the dict), and makes your workflow more transparent and reproducible.\n",
        "\n",
        "We want to automatically load the modelâ€™s own image processor so our pictures get resized, normalized, and turned into tensors exactly the way the ViT expectsâ€”saving us from having to write and debug all that preprocessing by hand. Let's create an image processor automatically from the [preprocessor configuration](https://huggingface.co/google/vit-base-patch16-224/blob/main/preprocessor_config.json) specified by the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umlWkRm3pyM8"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint, use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqWZMSzRpyM8"
      },
      "source": [
        "We can now prepare the preprocessing pipeline to transform the images in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpysvLRzpyM8"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "preprocess_pipeline = Compose([\n",
        "    Resize(image_processor.size[\"height\"]),\n",
        "    CenterCrop(image_processor.size[\"height\"]),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
        "])\n",
        "\n",
        "def preprocess(batch):\n",
        "    batch[\"pixel_values\"] = [\n",
        "        preprocess_pipeline(image.convert(\"RGB\")) for image in batch[\"image\"]\n",
        "    ]\n",
        "    return batch\n",
        "\n",
        "\n",
        "# Let's set the transform function to every train and test sets\n",
        "for cfg in config.values():\n",
        "    cfg[\"train_data\"].set_transform(preprocess)\n",
        "    cfg[\"test_data\"].set_transform(preprocess)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that is is done, we will fine tune the model."
      ],
      "metadata": {
        "id": "LIq1httYW78J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DxckFC8pyM8"
      },
      "source": [
        "## Fine-Tuning the Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are functions that we'll need to fine-tune the model.\n",
        "- `data_collate(examples)` : This function takes a list of individual examples (each containing \"pixel_values\" and \"label\") and stacks them into batched tensors: a pixel_values tensor of shape (batch_size, C, H, W) and a labels tensor of shape (batch_size,). Its purpose is to produce the correctly formatted input dict that the Hugging Face Trainer expects for both training and evaluation.\n",
        "- `compute_metrics(eval_pred)` : Given an EvalPrediction object with raw model logits and true label IDs, it selects the highestâ€scoring class via argmax and then computes accuracy against the references using the evaluate library. You plug this function into your Trainer so that it reports accuracy automatically at each evaluation step.\n",
        "- `get_base_model(label2id, id2label)` : This function loads the pretrained Vision Transformer classifier from the specified checkpoint, injecting your datasetâ€™s label2id and id2label mappings and allowing mismatched head sizes to be resized. It gives you a readyâ€toâ€use base model configured for your specific classification labels.\n",
        "- `build_lora_model(label2id, id2label)` : First, it calls get_base_model to instantiate the ViT classifier and prints how many parameters would be trainable if you fineâ€tuned all of them. Then it creates a LoraConfig (specifying lowâ€rank projection size, dropout, target modules, etc.), wraps the base model with LoRA adapters via get_peft_model, prints the new trainableâ€parameter count (just the adapters), and returns the lightweight LoRAâ€augmented model ready for efficient fineâ€tuning.\n"
      ],
      "metadata": {
        "id": "zGBGSwdNW7Im"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2Sy5ergpyM8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import evaluate\n",
        "import torch\n",
        "from peft import PeftModel, LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "\n",
        "def data_collate(examples):\n",
        "    \"\"\"\n",
        "    Prepare a batch of examples from a list of elements of the\n",
        "    train or test datasets.\n",
        "    \"\"\"\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Compute the model's accuracy on a batch of predictions.\n",
        "    \"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "\n",
        "def get_base_model(label2id, id2label):\n",
        "    \"\"\"\n",
        "    Create an image classification base model from\n",
        "    the model checkpoint.\n",
        "    \"\"\"\n",
        "    return AutoModelForImageClassification.from_pretrained(\n",
        "        model_checkpoint,\n",
        "        label2id=label2id,\n",
        "        id2label=id2label,\n",
        "        ignore_mismatched_sizes=True,\n",
        "    )\n",
        "\n",
        "\n",
        "def build_lora_model(label2id, id2label):\n",
        "    \"\"\"Build the LoRA model to fine-tune the base model.\"\"\"\n",
        "    model = get_base_model(label2id, id2label)\n",
        "    print_trainable_parameters(model, label=\"Base model\")\n",
        "\n",
        "    config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"query\", \"value\"],\n",
        "        lora_dropout=0.1,\n",
        "        bias=\"none\",\n",
        "        modules_to_save=[\"classifier\"],\n",
        "    )\n",
        "\n",
        "    lora_model = get_peft_model(model, config)\n",
        "    print_trainable_parameters(lora_model, label=\"LoRA\")\n",
        "\n",
        "    return lora_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmck-YTOpyM9"
      },
      "source": [
        "Let's now configure the fine-tuning process. Before kicking off training, we create a `TrainingArguments` object that tells the Hugging Face `Trainer` how, when, and with what settings to run our fine-tuning job:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHojD4vupyM9"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "batch_size = 128\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=\"./model-checkpoints\", #Directory where checkpoints and the final model are saved.\n",
        "    remove_unused_columns=False, #Keeps all dataset columns (e.g. \"pixel_values\" and \"labels\"), even if some arenâ€™t used by the modelâ€™s forward().\n",
        "    eval_strategy=\"epoch\", # Runs evaluation once at the end of every epoch.\n",
        "    save_strategy=\"epoch\", # Saves a model checkpoint after each epoch finishes.\n",
        "    learning_rate=5e-3, # Initial optimizer learning rateâ€”higher than usual since LoRA adapters converge quickly.\n",
        "    per_device_train_batch_size=batch_size, # Number of training examples per device (GPU/CPU) per step (128 here).\n",
        "    per_device_eval_batch_size=batch_size, # Number of evaluation examples per device per step (128 here).\n",
        "    gradient_accumulation_steps=4, # Accumulates gradients over 4 forward/backward passes before updatingâ€”simulating a batch size of 512 without extra memory.\n",
        "    fp16=True, # Enables mixed-precision (half-precision) training to speed up computation and reduce memory usage.\n",
        "    logging_steps=10, # Logs training loss and throughput metrics every 10 steps.\n",
        "    load_best_model_at_end=True, # After training, automatically reloads the checkpoint that achieved the best validation metric.\n",
        "    metric_for_best_model=\"accuracy\", # Uses validation accuracy to decide which checkpoint is â€œbest.â€\n",
        "    label_names=[\"labels\"], # Tells the trainer which field(s) in your batch dictionary contain the ground-truth labels.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLzljP5TpyM9"
      },
      "source": [
        "These settings give you:\n",
        "- Regular evaluation and checkpointing to monitor progress.\n",
        "- Memory-efficient training (mixed precision + gradient accumulation).\n",
        "- Automatic best-model selection based on validation accuracy.\n",
        "\n",
        "Let's now fine-tune both models. We loop over our `config` dict to train each LoRA-augmented ViT on its respective dataset, evaluate its performance, and save the adapter.\n",
        "We need now to automate the process: â€œfor each model configuration â†’ build the model â†’ train it â†’ evaluate its performance â†’ save the lightweight adapter and report its size.â€ That way you get two fully fine-tuned, evaluated, and saved adapters with just a few lines of code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvbTuggCpyM9"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "for cfg in config.values(): # Iterates through each modelâ€™s settings (model1, then model2).\n",
        "    # 1. Set the number of epochs for this experiment\n",
        "    training_arguments.num_train_epochs = cfg[\"epochs\"] # Dynamically updates the .num_train_epochs field so each model trains for its specified number of epochs.\n",
        "\n",
        "    # 2. Instantiate the Trainer\n",
        "    trainer = Trainer(\n",
        "        build_lora_model(cfg[\"label2id\"], cfg[\"id2label\"]), # Builds a fresh LoRA-wrapped ViT using build_lora_model(...).\n",
        "        training_arguments, # Supplies our TrainingArguments (checkpointing, mixed precision, etc.).\n",
        "        train_dataset=cfg[\"train_data\"], # Pass in the preprocessed HF Datasets.\n",
        "        eval_dataset=cfg[\"test_data\"], # Pass in the preprocessed HF Datasets.\n",
        "        tokenizer=image_processor, # Here we use the AutoImageProcessor as the collating/tokenizing function.\n",
        "        compute_metrics=compute_metrics, # Hook to compute and log accuracy after each evaluation.\n",
        "        data_collator=data_collate, # Custom collator that packages pixel tensors and labels into batches.\n",
        "    )\n",
        "\n",
        "    # 3. Run training\n",
        "    results = trainer.train() # Executes the training loop, saving checkpoints per epoch as configured.\n",
        "\n",
        "    # 4. Evaluate on the test split\n",
        "    evaluation_results = trainer.evaluate(cfg['test_data']) # Runs inference on the test split and returns metrics like eval_accuracy.\n",
        "    print(f\"Evaluation accuracy: {evaluation_results['eval_accuracy']}\")\n",
        "\n",
        "    # 5. Save the fine-tuned adapter and report its size\n",
        "    # We can now save the fine-tuned model to disk.\n",
        "    trainer.save_model(cfg[\"path\"]) # Exports the LoRA adapter (and base weights) to the specified folder.\n",
        "    print_model_size(cfg[\"path\"]) # Reports the on-disk size of the saved adapter, demonstrating the parameter-efficiency of LoRA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WddGcEL4pyM9"
      },
      "source": [
        "## Running Inference\n",
        "\n",
        "This block defines two functionsâ€”`build_inference_model` and `predict`â€”that together let you load a fine-tuned LoRA adapter on top of the base ViT model and then run it on new images to obtain human-readable class predictions.\n",
        "We will create two functions :  \n",
        "- `build_inference_model`:\n",
        "    - Loads the base ViT classifier configured for your datasetâ€™s labels.\n",
        "    - Wraps it with the LoRA adapter weights you fine-tuned and saved.\n",
        "\n",
        "- `predict`:\n",
        "    - Preprocesses any new PIL image exactly the way the model expects.\n",
        "    - Runs a forward pass to get predicted logits.\n",
        "    - Chooses the highest-scoring class and returns the human-readable label.\n",
        "\n",
        "Together, these functions let you deploy your fine-tuned LoRA adapters: you simply call build_inference_model(...) once to load the model, then repeatedly call predict(image, model, image_processor) on new images to get quick, accurate classifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWVM3DKppyM9"
      },
      "outputs": [],
      "source": [
        "def build_inference_model(label2id, id2label, lora_adapter_path):\n",
        "    \"\"\"Build the model that will be used to run inference.\"\"\"\n",
        "    # Load the base Vision Transformer with correct label mappings\n",
        "    model = get_base_model(label2id, id2label)\n",
        "    # Apply the saved LoRA adapter weights on top of the base model\n",
        "    return PeftModel.from_pretrained(model, lora_adapter_path)\n",
        "\n",
        "\n",
        "def predict(image, model, image_processor):\n",
        "    \"\"\"Predict the class represented by the supplied image.\"\"\"\n",
        "    # Convert the input PIL image to RGB and run it through the modelâ€™s image processor,\n",
        "    # returning a PyTorch tensor dictionary (e.g. {\"pixel_values\": ...})\n",
        "    encoding = image_processor(image.convert(\"RGB\"), return_tensors=\"pt\")\n",
        "    # Disable gradient tracking since weâ€™re only doing a forward pass\n",
        "    with torch.no_grad():\n",
        "        # Forward the processed image through the model to get raw outputs\n",
        "        outputs = model(**encoding)\n",
        "        # Extract the logits (unnormalized class scores)\n",
        "        logits = outputs.logits\n",
        "    # Select the class index with the highest logit score\n",
        "    class_index = logits.argmax(-1).item()\n",
        "    # Convert that index back to the original label string\n",
        "    return model.config.id2label[class_index]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcH4o-I2pyM-"
      },
      "source": [
        "Now, we need to create two inference models, one using each of the LoRA adapters. Here we loop over our `config` entries to load two separate inference-ready models (one per LoRA adapter) and the matching image processor for each.  This loop prepares everything you need for deploymentâ€”inference-ready models and their matching preprocessorsâ€”so that downstream code can simply reference cfg[\"inference_model\"] and cfg[\"image_processor\"] to run predictions on new images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qN0OoU5pyM-"
      },
      "outputs": [],
      "source": [
        "for cfg in config.values():\n",
        "    # Load the base ViT + the fine-tuned LoRA adapter from disk\n",
        "    cfg[\"inference_model\"] = build_inference_model(\n",
        "        cfg[\"label2id\"],      # mapping from string labels â†’ IDs\n",
        "        cfg[\"id2label\"],      # mapping from IDs â†’ string labels\n",
        "        cfg[\"path\"]           # path where the LoRA adapter is saved\n",
        "    )\n",
        "\n",
        "    # Load the exact image processor (preprocessor config) saved with that adapter,\n",
        "    # so it uses the same resize/normalize settings as during training\n",
        "    cfg[\"image_processor\"] = AutoImageProcessor.from_pretrained(cfg[\"path\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmENUixHpyM-"
      },
      "source": [
        "Here is a list of sample images and the model that we need to use to"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TkzjRXQpyM-"
      },
      "outputs": [],
      "source": [
        "samples = [\n",
        "    {\n",
        "        \"image\": \"__opt__aboutcom__coeus__resources__content_migration__simply_recipes__uploads__2019__09__easy-pepperoni-pizza-lead-3-1024x682-583b275444104ef189d693a64df625da.jpg\",\n",
        "        \"model\": \"model1\",\n",
        "    },\n",
        "    {\n",
        "        \"image\": \"https://wallpapers.com/images/featured/kitty-cat-pictures-nzlg8fu5sqx1m6qj.jpg\",\n",
        "        \"model\": \"model2\",\n",
        "    },\n",
        "    {\n",
        "        \"image\": \"https://i.natgeofe.com/n/5f35194b-af37-4f45-a14d-60925b280986/NationalGeographic_2731043_3x4.jpg\",\n",
        "        \"model\": \"model2\",\n",
        "    },\n",
        "    {\n",
        "        \"image\": \"https://www.simplyrecipes.com/thmb/KE6iMblr3R2Db6oE8HdyVsFSj2A=/1500x0/filters:no_upscale():max_bytes(150000):strip_icc()/__opt__aboutcom__coeus__resources__content_migration__simply_recipes__uploads__2019__09__easy-pepperoni-pizza-lead-3-1024x682-583b275444104ef189d693a64df625da.jpg\",\n",
        "        \"model\": \"model1\"\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlkjXEtHpyM-"
      },
      "source": [
        "We can now run predictions on every sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UORUvxAGpyM-"
      },
      "outputs": [],
      "source": [
        "from PIL import Image         # Import PIL for image loading and manipulation\n",
        "import requests                # Import requests to fetch images from URLs\n",
        "\n",
        "for sample in samples:\n",
        "    # 1. Download the image from its URL and open it as a PIL.Image\n",
        "    image = Image.open(\n",
        "        requests.get(sample[\"image\"], stream=True).raw\n",
        "    )\n",
        "\n",
        "    # 2. Retrieve the correct inference model & processor for this sample\n",
        "    inference_model   = config[sample[\"model\"]][\"inference_model\"]\n",
        "    image_processor   = config[sample[\"model\"]][\"image_processor\"]\n",
        "\n",
        "    # 3. Run the image through our `predict` helper to get a label\n",
        "    prediction = predict(image, inference_model, image_processor)\n",
        "\n",
        "    # 4. Print out the result\n",
        "    print(f\"Prediction: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qChR927FOz98"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}