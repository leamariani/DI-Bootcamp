{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "18a5ac71",
      "metadata": {
        "id": "18a5ac71"
      },
      "source": [
        "# Daily Challenge — Construire un chatbot avec Transformers et Gradio\n",
        "\n",
        "Ce notebook montre comment créer un chatbot conversationnel avec la bibliothèque Transformers de Hugging Face et une interface web Gradio. Le modèle utilisé est `facebook/blenderbot-400M-distill`.\n",
        "\n",
        "## Objectifs\n",
        "- Charger et utiliser un modèle conversationnel avec `transformers`.\n",
        "- Maintenir le contexte via la classe `Conversation`.\n",
        "- Construire une interface interactive avec Gradio.\n",
        "- Simuler une conversation en temps réel en Python.\n",
        "\n",
        "## Choix techniques\n",
        "- Le pipeline conversational simplifie l’inférence et encapsule le modèle et le tokenizer.\n",
        "- La classe Conversation permet de conserver l’historique des tours de dialogue.\n",
        "- Gradio ChatInterface fournit une interface prête à l’emploi pour discuter avec le modèle."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7a1551b",
      "metadata": {
        "id": "a7a1551b"
      },
      "source": [
        "## Étape 0 — Installation et import des bibliothèques\n",
        "\n",
        "Cette cellule importe `transformers` et `gradio`. Si nécessaire, elle installe les dépendances. L’installation peut prendre du temps car les poids du modèle seront téléchargés lors du premier chargement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "de6cc23e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de6cc23e",
        "outputId": "378a4a44-0a96-4478-df46-7866d171a31d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.55.2\n",
            "Gradio version: 5.42.0\n"
          ]
        }
      ],
      "source": [
        "def _safe_imports():\n",
        "    try:\n",
        "        import transformers  # noqa: F401\n",
        "        import gradio  # noqa: F401\n",
        "        return True\n",
        "    except Exception:\n",
        "        import sys, subprocess\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\", \"gradio\", \"-q\"])\n",
        "        return True\n",
        "\n",
        "_safe_imports()\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "import transformers as _tf\n",
        "print(\"Transformers version:\", _tf.__version__)\n",
        "print(\"Gradio version:\", gr.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42f84b92",
      "metadata": {
        "id": "42f84b92"
      },
      "source": [
        "## Étape 1 — Charger le pipeline conversationnel\n",
        "\n",
        "Nous chargeons un pipeline de type `conversational` avec le modèle `facebook/blenderbot-400M-distill`. Ce pipeline prend des objets `Conversation` en entrée et les enrichit avec les réponses générées."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "70804c87",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70804c87",
        "outputId": "acc64760-fdf1-4303-a2d3-9637598f3204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and Tokenizer loaded: facebook/blenderbot-400M-distill\n"
          ]
        }
      ],
      "source": [
        "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
        "\n",
        "MODEL_NAME = \"facebook/blenderbot-400M-distill\"\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "print(\"Model and Tokenizer loaded:\", MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fc1fc4f",
      "metadata": {
        "id": "2fc1fc4f"
      },
      "source": [
        "## Étape 2 — Démarrer et étendre une conversation (console)\n",
        "\n",
        "Nous créons une instance `Conversation` avec un message utilisateur initial, puis nous la passons au pipeline pour générer une réponse. Nous ajoutons ensuite un nouvel input utilisateur et relançons le pipeline. L’objet `Conversation` conserve l’historique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "bed542aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bed542aa",
        "outputId": "04879c26-a555-4093-ffc9-8a1258d4849b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tour 1:\n",
            "User: Hi, I am testing a BlenderBot chatbot. How are you today?\n",
            "Bot:  I am doing well, how are you? I have never heard of that, what is it?\n",
            "\n",
            "Tour 2:\n",
            "User: What can you do? Give me a very short answer.\n",
            "Bot:  I'm doing great, thanks for asking. It's a chat app that allows you to talk to people over the internet.\n",
            "\n",
            "Full chat history: [['Hi, I am testing a BlenderBot chatbot. How are you today?', ' I am doing well, how are you? I have never heard of that, what is it?'], ['What can you do? Give me a very short answer.', \" I'm doing great, thanks for asking. It's a chat app that allows you to talk to people over the internet.\"]]\n"
          ]
        }
      ],
      "source": [
        "# Initialize chat history as an empty list of tuples\n",
        "chat_history = []\n",
        "\n",
        "# First turn\n",
        "user_input_1 = \"Hi, I am testing a BlenderBot chatbot. How are you today?\"\n",
        "response_1 = generate_response(user_input_1, chat_history)\n",
        "print(f\"Tour 1:\\nUser: {user_input_1}\\nBot: {response_1}\")\n",
        "chat_history.append([user_input_1, response_1])\n",
        "\n",
        "# Second turn\n",
        "user_input_2 = \"What can you do? Give me a very short answer.\"\n",
        "response_2 = generate_response(user_input_2, chat_history)\n",
        "print(f\"\\nTour 2:\\nUser: {user_input_2}\\nBot: {response_2}\")\n",
        "chat_history.append([user_input_2, response_2])\n",
        "\n",
        "print(\"\\nFull chat history:\", chat_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e18070c",
      "metadata": {
        "id": "7e18070c"
      },
      "source": [
        "## Étape 3 — Définir la fonction `mini_chatbot` pour Gradio\n",
        "\n",
        "La fonction prend le `message` utilisateur et un `history` fourni par Gradio (liste de paires `[user, bot]`). Nous reconstruisons une `Conversation` à partir de cet historique, y ajoutons le message courant, appelons le pipeline, puis retournons la dernière réponse générée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8e1ad800",
      "metadata": {
        "id": "8e1ad800"
      },
      "outputs": [],
      "source": [
        "def mini_chatbot(message, history):\n",
        "    if history is None:\n",
        "        history = []\n",
        "    # The conversational pipeline can handle the history directly\n",
        "    # We need to format the history into a format the pipeline understands\n",
        "    # The pipeline expects a list of dictionaries with 'user' and 'generated' keys\n",
        "    formatted_history = []\n",
        "    for user_msg, bot_msg in history:\n",
        "      formatted_history.append({'user': user_msg, 'generated': bot_msg})\n",
        "\n",
        "    # Pass the current message and formatted history to the chatbot\n",
        "    response = chatbot(message, chat_history=formatted_history)\n",
        "\n",
        "    # The response is a Conversation object, the latest generated response is the last one\n",
        "    return response.generated_responses[-1] if response.generated_responses else \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5755ae86",
      "metadata": {
        "id": "5755ae86"
      },
      "source": [
        "## Étape 4 — Construire l’interface Gradio\n",
        "\n",
        "Nous utilisons `gr.ChatInterface` qui prend une fonction de dialogue et gère l’historique automatiquement. Le titre et la description aident à documenter l’interface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "a3bfd654",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3bfd654",
        "outputId": "ce1197d7-ce2f-48e4-da95-f42f20a83a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        }
      ],
      "source": [
        "demo_chatbot = gr.ChatInterface(\n",
        "    fn=mini_chatbot,\n",
        "    title=\"Chatbot Transformers — BlenderBot 400M distill\",\n",
        "    description=\"Discutez avec un modèle BlenderBot distillé. L'historique de la conversation est conservé pour fournir du contexte.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "afbb0392",
      "metadata": {
        "id": "afbb0392"
      },
      "source": [
        "## Étape 5 — Lancer l’application\n",
        "\n",
        "Exécutez la cellule suivante pour démarrer l’interface. Une URL locale s’affiche. Ouvrez-la dans votre navigateur puis commencez à converser. Le premier échange peut être plus lent à cause du chargement des poids du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a784f247",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "a784f247",
        "outputId": "a7462c87-b56b-412a-c767-4927f60ca499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "* To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7860, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "demo_chatbot.launch(share=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c827e635",
      "metadata": {
        "id": "c827e635"
      },
      "source": [
        "## Bonnes pratiques et extensions possibles\n",
        "\n",
        "- Ajouter une limite de longueur de contexte pour éviter un historique trop volumineux.\n",
        "- Forcer une langue en prétraitant l’entrée, ou détecter la langue automatiquement.\n",
        "- Mettre en cache le pipeline ou charger le modèle sur GPU si disponible pour accélérer les réponses.\n",
        "- En production, ajouter une modération de contenu et une journalisation des erreurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9791c8ff"
      },
      "source": [
        "from transformers import BlenderbotForConditionalGeneration, BlenderbotTokenizer\n",
        "\n",
        "MODEL_NAME = \"facebook/blenderbot-400M-distill\"\n",
        "tokenizer = BlenderbotTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BlenderbotForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def generate_response(user_input, chat_history):\n",
        "    # Encode the conversation history and the new user input\n",
        "    # The model expects the history to be in a specific format, usually alternating user and bot messages\n",
        "    # We'll format the history as a single string\n",
        "    history_string = \"\"\n",
        "    for user_msg, bot_msg in chat_history:\n",
        "        history_string += f\"<s>{user_msg}</s>{bot_msg}</s>\"\n",
        "    history_string += f\"<s>{user_input}</s>\"\n",
        "\n",
        "    # Tokenize the input string\n",
        "    inputs = tokenizer([history_string], return_tensors='pt')\n",
        "\n",
        "    # Generate the response\n",
        "    reply_ids = model.generate(**inputs)\n",
        "\n",
        "    # Decode the generated response\n",
        "    response = tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0]\n",
        "    return response\n",
        "\n",
        "# Example usage:\n",
        "chat_history = []\n",
        "user_input = \"Hi, I am testing a BlenderBot chatbot. How are you today?\"\n",
        "response = generate_response(user_input, chat_history)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"Bot: {response}\")\n",
        "chat_history.append([user_input, response])\n",
        "\n",
        "user_input = \"What can you do? Give me a very short answer.\"\n",
        "response = generate_response(user_input, chat_history)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"Bot: {response}\")\n",
        "chat_history.append([user_input, response])\n",
        "\n",
        "print(\"\\nFull chat history:\", chat_history)"
      ],
      "id": "9791c8ff",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}