{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "866bd0dd",
   "metadata": {},
   "source": [
    "# Décryptage des difficultés des développeurs face aux LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d99bf",
   "metadata": {},
   "source": [
    "\n",
    "## Objectif\n",
    "\n",
    "Ce projet est basé sur l’article [“An Empirical Study on Challenges for LLM Application Developers”](https://dl.acm.org/doi/pdf/10.1145/3715007).  \n",
    "Il vise à :\n",
    "\n",
    "- Comprendre la méthodologie empirique utilisée\n",
    "- Analyser la structure de la taxonomie des défis liés aux LLM\n",
    "- Identifier les implications pratiques pour les plateformes et APIs LLM\n",
    "- Proposer des solutions concrètes sous forme d’outils ou de ressources communautaires\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5808c35e",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Méthodologie (Section 3)\n",
    "\n",
    "### Principales décisions méthodologiques\n",
    "\n",
    "- Les auteurs ont analysé **98 posts de développeurs** sur Reddit, GitHub, Stack Overflow et Hacker News.\n",
    "- La méthode utilisée est une **analyse qualitative thématique**, appuyée par une stratégie de **codage inductif**.\n",
    "- Les participants étaient des développeurs de LLMs expérimentés (projets open-source, outils internes).\n",
    "\n",
    "### Validité et fiabilité\n",
    "\n",
    "- Codage réalisé de façon **indépendante par deux chercheurs**, puis confrontation des catégories.\n",
    "- Utilisation de **l’approche par saturation** : le codage s’est arrêté lorsque plus aucun nouveau thème n’émergeait.\n",
    "- Les exemples et citations ont été validés par **triangulation des sources**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149dd57b",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Taxonomie des défis (Section 6)\n",
    "\n",
    "Voici la taxonomie des défis rencontrés par les développeurs utilisant des LLMs :\n",
    "\n",
    "### Taxonomie en hiérarchie :\n",
    "\n",
    "- **1. Prompt Engineering**\n",
    "  - Difficulté à concevoir des prompts efficaces\n",
    "  - Manque de généralisation des prompts\n",
    "- **2. Tooling & Infrastructure**\n",
    "  - Manque d’outils pour le débogage\n",
    "  - Limites des API et interfaces\n",
    "- **3. Evaluation & Testing**\n",
    "  - Absence de métriques fiables\n",
    "  - Evaluation subjective ou qualitative\n",
    "- **4. Cost & Performance**\n",
    "  - Coût élevé des appels LLM\n",
    "  - Latence et instabilité des réponses\n",
    "- **5. LLM Behavior**\n",
    "  - Hallucinations et incohérences\n",
    "  - Difficulté de contrôle du style ou du ton\n",
    "- **6. Security & Ethics**\n",
    "  - Biais et contenus offensants\n",
    "  - Risques de fuites de données\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf438ba",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Analyse et implications\n",
    "\n",
    "### Quels types de défis dominent le développement LLM ?\n",
    "\n",
    "Les défis dominants sont liés à **l’engineering de prompt**, **l’évaluation**, et **la compréhension du comportement des modèles**.  \n",
    "Les développeurs peinent à **maîtriser le comportement des LLMs**, à tester de manière fiable, et à obtenir des résultats consistants.\n",
    "\n",
    "### Implications pour le design des APIs / plateformes LLM\n",
    "\n",
    "- Les APIs doivent **intégrer des outils d’évaluation intégrés** (métriques, scoring subjectif + quantitatif)\n",
    "- Il est nécessaire d’offrir des **outils interactifs pour l’ingénierie de prompt** avec visualisation des variantes\n",
    "- Les plateformes doivent mieux **documenter les comportements non-déterministes** et **exposer les limites connues**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f59d87e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Idées d’outils ou ressources communautaires\n",
    "\n",
    "### 1. LLM Prompt Playground Collaboratif\n",
    "\n",
    "- Interface open-source où les développeurs peuvent tester des prompts, voir les résultats d’autres utilisateurs, et évaluer leur efficacité.\n",
    "- Historique, scoring collaboratif, suggestions automatisées, partage de \"recipes\".\n",
    "\n",
    "### 2. Guide vivant des erreurs LLM\n",
    "\n",
    "- Une base de connaissance partagée, type wiki, contenant les erreurs fréquentes (hallucinations, biais, incohérences) par type de modèle et cas d’usage.\n",
    "- Enrichie par la communauté et validée par des experts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c491e4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
