{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c9985571",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9985571",
        "outputId": "8e1ee237-10e6-4e51-cff2-49aa344598ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cells': [{'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['# Challenge : Construire un Agent RAG Auto-Correcteur avec LangGraph\\n',\n",
              "    '\\n',\n",
              "    \"Bienvenue dans ce notebook ! L'objectif est de construire un agent conversationnel sophistiqué en utilisant le framework **LangGraph**. \\n\",\n",
              "    '\\n',\n",
              "    \"Cet agent ne se contentera pas de répondre à des questions sur la base de documents fournis (le principe du RAG - Retrieval-Augmented Generation), mais il sera capable d'évaluer la pertinence des informations récupérées. Si les informations ne sont pas pertinentes, l'agent **reformulera la question** de l'utilisateur pour tenter d'obtenir de meilleurs résultats, créant ainsi une boucle de raisonnement et d'auto-correction.\\n\",\n",
              "    '\\n',\n",
              "    '## Plan du Notebook\\n',\n",
              "    '\\n',\n",
              "    \"1.  **Configuration de l'Environnement** : Chargement des clés API et des variables d'environnement.\\n\",\n",
              "    '2.  **Construction de la Base de Connaissances (Le \"R\" de RAG)** : Chargement de documents depuis le web, découpage, et création d\\'un index vectoriel avec ChromaDB.\\n',\n",
              "    '3.  **Définition de l\\'État de l\\'Agent** : Création de la structure de données qui servira de \"mémoire\" à notre agent tout au long du processus.\\n',\n",
              "    '4.  **Création des Outils et des Nœuds du Graphe** : Définition des différentes fonctions qui représenteront les étapes de notre agent (recherche, évaluation, réécriture, génération).\\n',\n",
              "    \"5.  **Assemblage du Graphe avec LangGraph** : Connexion des nœuds avec une logique conditionnelle pour orchestrer le flux de travail de l'agent.\\n\",\n",
              "    \"6.  **Compilation et Test** : Création de l'application exécutable et validation de son comportement.\\n\",\n",
              "    \"7.  **Fonction d'Interface pour Streamlit** : Création d'une fonction `run_agent` simple que notre application Streamlit pourra appeler.\"]},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': [\"### Étape 1 : Configuration de l'Environnement\\n\",\n",
              "    '\\n',\n",
              "    \"La première étape consiste à s'assurer que notre environnement est correctement configuré. Nous allons charger les bibliothèques nécessaires et les clés API à partir d'un fichier `.env`. C'est une pratique essentielle pour garder nos informations sensibles (comme les clés API) séparées de notre code.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['import os\\n',\n",
              "    'from dotenv import load_dotenv\\n',\n",
              "    '\\n',\n",
              "    \"# Charger les variables d'environnement du fichier .env\\n\",\n",
              "    '# Cela permet à os.getenv() de trouver les clés que nous avons stockées.\\n',\n",
              "    'load_dotenv()\\n',\n",
              "    '\\n',\n",
              "    '# Configuration optionnelle pour le traçage avec LangSmith\\n',\n",
              "    '# LangSmith est un outil fantastique pour visualiser et déboguer les chaînes et agents LangChain.\\n',\n",
              "    \"# C'est très utile pour comprendre ce qui se passe sous le capot de notre graphe.\\n\",\n",
              "    'os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\\n',\n",
              "    'os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\\n',\n",
              "    '# Assurez-vous que LANGCHAIN_API_KEY est défini dans votre fichier .env si vous souhaitez utiliser LangSmith\\n',\n",
              "    '\\n',\n",
              "    '# Récupération des clés API\\n',\n",
              "    'GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\\n',\n",
              "    'GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") # Utilisé pour les embeddings\\n',\n",
              "    '\\n',\n",
              "    'print(\"Clés API chargées.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['### Étape 2 : Construction de la Base de Connaissances (Le \"R\" de RAG)\\n',\n",
              "    '\\n',\n",
              "    \"Un agent RAG a besoin d'une base de connaissances sur laquelle s'appuyer. Nous allons construire cette base en plusieurs sous-étapes :\\n\",\n",
              "    '\\n',\n",
              "    '1.  **Chargement des données** : Nous utiliserons `WebBaseLoader` de LangChain pour charger le contenu de quelques articles de blog pertinents sur le Machine Learning et les agents.\\n',\n",
              "    '2.  **Découpage des documents** : Les documents bruts sont souvent trop longs pour être traités efficacement par un LLM. Nous les découperons en plus petits morceaux (chunks) avec `RecursiveCharacterTextSplitter`. Cette méthode est robuste car elle essaie de couper le texte sur des séparateurs logiques (paragraphes, phrases, etc.).\\n',\n",
              "    \"3.  **Création des Embeddings** : Pour que la machine comprenne le sens sémantique de nos morceaux de texte, nous devons les convertir en vecteurs numériques (embeddings). Nous utiliserons un modèle d'embedding de Google.\\n\",\n",
              "    \"4.  **Stockage dans un Vectorstore** : Ces vecteurs seront stockés et indexés dans une base de données vectorielle, **ChromaDB**. Cela nous permettra d'effectuer des recherches de similarité rapides : trouver les morceaux de texte les plus pertinents pour une question donnée.\\n\",\n",
              "    \"5.  **Création de l'outil de recherche (Retriever Tool)** : Enfin, nous encapsulerons notre logique de recherche dans un `Tool` LangChain. Cela permettra à notre agent d'appeler la recherche dans la base de connaissances comme n'importe quel autre outil.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['from langchain_community.document_loaders import WebBaseLoader\\n',\n",
              "    'from langchain.text_splitter import RecursiveCharacterTextSplitter\\n',\n",
              "    'from langchain_community.vectorstores import Chroma\\n',\n",
              "    'from langchain_google_genai import GoogleGenerativeAIEmbeddings\\n',\n",
              "    'from langchain.tools.retriever import create_retriever_tool\\n',\n",
              "    '\\n',\n",
              "    '# 1. Chargement des données\\n',\n",
              "    '# Nous choisissons des URLs qui parlent des agents et du RAG pour que notre agent soit bien informé sur ce sujet.\\n',\n",
              "    'urls = [\\n',\n",
              "    '    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\\n',\n",
              "    '    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\\n',\n",
              "    '    \"https://lilianweng.github.io/posts/2023-10-25-adv-rag/\",\\n',\n",
              "    ']\\n',\n",
              "    '\\n',\n",
              "    'loader = WebBaseLoader(urls)\\n',\n",
              "    'docs = loader.load()\\n',\n",
              "    'print(f\"{len(docs)} documents chargés.\")\\n',\n",
              "    '\\n',\n",
              "    '# 2. Découpage des documents\\n',\n",
              "    '# chunk_size définit la taille maximale de chaque morceau.\\n',\n",
              "    \"# chunk_overlap conserve une petite partie de la fin d'un morceau au début du suivant pour préserver le contexte.\\n\",\n",
              "    'text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\n',\n",
              "    'splits = text_splitter.split_documents(docs)\\n',\n",
              "    'print(f\"{len(splits)} morceaux créés.\")\\n',\n",
              "    '\\n',\n",
              "    '# 3. Création des Embeddings\\n',\n",
              "    '# Nous utilisons un modèle de Google. Il est performant et bien intégré avec LangChain.\\n',\n",
              "    'embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\\n',\n",
              "    '\\n',\n",
              "    '# 4. Stockage dans un Vectorstore\\n',\n",
              "    '# Chroma est une base de données vectorielle open-source et légère, parfaite pour des projets comme celui-ci.\\n',\n",
              "    '# \"from_documents\" s\\'occupe de générer les embeddings et de les stocker.\\n',\n",
              "    'vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\\n',\n",
              "    '\\n',\n",
              "    \"# 5. Création de l'outil de recherche\\n\",\n",
              "    '# Nous transformons notre vectorstore en un \"retriever\", qui est un objet capable de récupérer des documents.\\n',\n",
              "    'retriever = vectorstore.as_retriever()\\n',\n",
              "    '\\n',\n",
              "    '# Enfin, nous créons un \"Tool\". L\\'agent utilisera le nom et la description pour décider quand utiliser cet outil.\\n',\n",
              "    'retriever_tool = create_retriever_tool(\\n',\n",
              "    '    retriever,\\n',\n",
              "    '    name=\"retrieval_tool\",\\n',\n",
              "    '    description=\"Recherche des informations sur les agents d\\'IA, le RAG et le prompt engineering. Utilise cet outil pour répondre aux questions sur le machine learning.\"\\n',\n",
              "    ')\\n',\n",
              "    '\\n',\n",
              "    'tools = [retriever_tool]\\n',\n",
              "    'print(\"Outil de recherche créé et prêt à l\\'emploi.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': [\"### Étape 3 : Définition de l'État de l'Agent\\n\",\n",
              "    '\\n',\n",
              "    'LangGraph fonctionne comme une **machine à états**. Nous devons définir explicitement la structure des données qui persisteront et seront modifiées à travers les différents nœuds de notre graphe. C\\'est ce que nous appelons l\\'\"État\" (`State`).\\n',\n",
              "    '\\n',\n",
              "    \"Notre état, que nous nommerons `AgentState`, contiendra l'historique de la conversation. Nous utilisons `TypedDict` pour définir clairement les champs et leurs types. `Annotated[list[BaseMessage], operator.add]` est une syntaxe spéciale de LangGraph qui indique que le champ `messages` est une liste de messages et que les nouvelles valeurs doivent être ajoutées à la liste existante, plutôt que de la remplacer.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['from typing import TypedDict, Annotated, List\\n',\n",
              "    'from langchain_core.messages import BaseMessage\\n',\n",
              "    'import operator\\n',\n",
              "    '\\n',\n",
              "    'class AgentState(TypedDict):\\n',\n",
              "    '    \"\"\"\\n',\n",
              "    \"    Définit la structure de l'état de notre agent.\\n\",\n",
              "    '    \\n',\n",
              "    '    Attributes:\\n',\n",
              "    \"        messages: L'historique des messages de la conversation.\\n\",\n",
              "    \"                  L'annotation `operator.add` signifie que les nouveaux messages\\n\",\n",
              "    '                  seront ajoutés à la liste existante à chaque étape.\\n',\n",
              "    '    \"\"\"\\n',\n",
              "    '    messages: Annotated[list[BaseMessage], operator.add]\\n',\n",
              "    '\\n',\n",
              "    'print(\"État de l\\'agent défini.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['### Étape 4 : Création des Outils et des Nœuds du Graphe\\n',\n",
              "    '\\n',\n",
              "    \"C'est le cœur de notre agent. Nous allons définir plusieurs fonctions Python qui serviront de **nœuds** dans notre graphe. Chaque nœud effectue une tâche spécifique.\\n\",\n",
              "    '\\n',\n",
              "    '1.  **Modèle LLM** : Nous choisissons le modèle qui servira de \"cerveau\" à notre agent. Nous utilisons `ChatGroq` pour sa rapidité.\\n',\n",
              "    \"2.  **Nœud `My_AI_Assistant` (Routeur)** : C'est le point d'entrée. Ce nœud examine la dernière question de l'utilisateur et, en utilisant le LLM et la description des outils disponibles, décide s'il doit appeler un outil (notre `retrieval_tool`) ou s'il peut répondre directement.\\n\",\n",
              "    \"3.  **Nœud `retrieve` (Action)** : Ce nœud est un `ToolNode`. Il est responsable de l'exécution effective des appels d'outils décidés par le routeur.\\n\",\n",
              "    '4.  **Fonction `grade_documents` (Aiguilleur Conditionnel)** : Ce n\\'est pas un nœud, mais une fonction qui sera utilisée pour créer une **arête conditionnelle**. Elle prend les documents récupérés, demande à un LLM de les noter (sont-ils pertinents ?), et retourne une décision (\"generate\" ou \"rewrite\"). Pour garantir une sortie fiable, nous utilisons les **sorties structurées** de LangChain, forçant le LLM à répondre dans un format JSON que nous définissons.\\n',\n",
              "    \"5.  **Nœud `generate` (Générateur de réponse)** : Ce nœud prend les documents jugés pertinents et la question de l'utilisateur, et génère la réponse finale.\\n\",\n",
              "    \"6.  **Nœud `rewrite` (Réécrivain de question)** : Si les documents ne sont pas pertinents, ce nœud est appelé. Il prend la question originale et demande au LLM de la reformuler pour qu'elle soit plus claire ou plus spécifique, dans l'espoir d'obtenir de meilleurs résultats de recherche.\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['from langchain_groq import ChatGroq\\n',\n",
              "    'from langgraph.prebuilt import ToolNode\\n',\n",
              "    'from langchain_core.pydantic_v1 import BaseModel, Field\\n',\n",
              "    'from langchain_core.output_parsers import JsonOutputParser\\n',\n",
              "    'from langchain_core.prompts import PromptTemplate\\n',\n",
              "    'from langchain_core.messages import HumanMessage, ToolMessage\\n',\n",
              "    '\\n',\n",
              "    '# 1. Modèle LLM\\n',\n",
              "    \"# Nous utilisons Groq pour sa vitesse d'inférence impressionnante.\\n\",\n",
              "    '# Le modèle Llama3 8b est un bon compromis entre performance et rapidité.\\n',\n",
              "    'llm = ChatGroq(model=\"llama3-8b-8192\", temperature=0)\\n',\n",
              "    '\\n',\n",
              "    '# Lier les outils au LLM. Cela permet au LLM de savoir quels outils il peut appeler.\\n',\n",
              "    'llm_with_tools = llm.bind_tools(tools)\\n',\n",
              "    '\\n',\n",
              "    '# 2. Nœud `My_AI_Assistant` (Routeur)\\n',\n",
              "    'def ai_assistant(state: AgentState):\\n',\n",
              "    '    \"\"\"Point d\\'entrée : décide s\\'il faut appeler un outil ou non.\"\"\"\\n',\n",
              "    '    print(\"---ASSISTANT (ROUTEUR)---\")\\n',\n",
              "    '    messages = state[\"messages\"]\\n',\n",
              "    \"    # Appel au LLM avec la capacité d'utiliser des outils\\n\",\n",
              "    '    response = llm_with_tools.invoke(messages)\\n',\n",
              "    \"    # La réponse est ajoutée à l'état, qu'elle contienne un appel d'outil ou une réponse directe.\\n\",\n",
              "    '    return {\"messages\": [response]}\\n',\n",
              "    '\\n',\n",
              "    '# 3. Nœud `retrieve` (Action)\\n',\n",
              "    '# ToolNode est un nœud pré-construit qui exécute les outils appelés par le LLM.\\n',\n",
              "    'retrieve = ToolNode(tools)\\n',\n",
              "    '\\n',\n",
              "    '# 4. Fonction `grade_documents` (Aiguilleur Conditionnel)\\n',\n",
              "    '\\n',\n",
              "    \"# Définition du format de sortie pour l'évaluation\\n\",\n",
              "    'class Grade(BaseModel):\\n',\n",
              "    '    \"\"\"Évaluation binaire de la pertinence des documents.\"\"\"\\n',\n",
              "    '    binary_score: str = Field(description=\"Les documents sont-ils pertinents ? \\'oui\\' ou \\'non\\'\")\\n',\n",
              "    '\\n',\n",
              "    'def grade_documents(state: AgentState):\\n',\n",
              "    '    \"\"\"Évalue si les documents récupérés sont pertinents pour la question.\"\"\"\\n',\n",
              "    '    print(\"---ÉVALUATION DES DOCUMENTS---\")\\n',\n",
              "    '    messages = state[\"messages\"]\\n',\n",
              "    '    last_message = messages[-1]\\n',\n",
              "    '    question = messages[0].content\\n',\n",
              "    '\\n',\n",
              "    '    # Créer un parser pour la sortie structurée\\n',\n",
              "    '    parser = JsonOutputParser(pydantic_object=Grade)\\n',\n",
              "    '\\n',\n",
              "    '    # Créer un LLM avec la sortie structurée\\n',\n",
              "    '    structured_llm_grader = llm.with_structured_output(Grade)\\n',\n",
              "    '\\n',\n",
              "    \"    # Prompt pour l'évaluation\\n\",\n",
              "    '    prompt = PromptTemplate(\\n',\n",
              "    '        template=\"Vous êtes un évaluateur qui note la pertinence des documents récupérés par rapport à une question utilisateur.\\\\n\"\\n',\n",
              "    '                 \"Voici les documents récupérés : \\\\n\\\\n {documents} \\\\n\\\\n\"\\n',\n",
              "    '                 \"Voici la question de l\\'utilisateur : {question} \\\\n\"\\n',\n",
              "    '                 \"Si les documents contiennent des mots-clés ou des concepts sémantiques liés à la question, notez-les comme pertinents.\\\\n\"\\n',\n",
              "    '                 \"Donnez une note binaire, \\'oui\\' ou \\'non\\', pour indiquer si les documents sont pertinents.\",\\n',\n",
              "    '        input_variables=[\"question\", \"documents\"],\\n',\n",
              "    '    )\\n',\n",
              "    '\\n',\n",
              "    '    chain = prompt | structured_llm_grader\\n',\n",
              "    '    docs = last_message.content\\n',\n",
              "    '    response = chain.invoke({\"question\": question, \"documents\": docs})\\n',\n",
              "    '\\n',\n",
              "    '    if response.binary_score == \"oui\":\\n',\n",
              "    '        print(\"Décision : Documents pertinents. Passage à la génération.\")\\n',\n",
              "    '        return \"generate\"\\n',\n",
              "    '    else:\\n',\n",
              "    '        print(\"Décision : Documents non pertinents. Passage à la réécriture.\")\\n',\n",
              "    '        return \"rewrite\"\\n',\n",
              "    '\\n',\n",
              "    '# 5. Nœud `generate` (Générateur de réponse)\\n',\n",
              "    'def generate(state: AgentState):\\n',\n",
              "    '    \"\"\"Génère une réponse finale en utilisant les documents et la question.\"\"\"\\n',\n",
              "    '    print(\"---GÉNÉRATION DE LA RÉPONSE---\")\\n',\n",
              "    '    messages = state[\"messages\"]\\n',\n",
              "    '    question = messages[0].content\\n',\n",
              "    '    last_message = messages[-1]\\n',\n",
              "    '    docs = last_message.content\\n',\n",
              "    '\\n',\n",
              "    '    prompt = f\"Vous êtes un assistant IA spécialisé dans le Machine Learning. Répondez à la question de l\\'utilisateur en vous basant sur le contexte suivant :\\\\n\\\\nContexte : {docs}\\\\n\\\\nQuestion : {question}\\\\n\\\\nRéponse :\"\\n',\n",
              "    '    \\n',\n",
              "    '    response = llm.invoke(prompt)\\n',\n",
              "    '    return {\"messages\": [response]}\\n',\n",
              "    '\\n',\n",
              "    '# 6. Nœud `rewrite` (Réécrivain de question)\\n',\n",
              "    'def rewrite(state: AgentState):\\n',\n",
              "    '    \"\"\"Reformule la question de l\\'utilisateur pour une meilleure recherche.\"\"\"\\n',\n",
              "    '    print(\"---RÉÉCRITURE DE LA QUESTION---\")\\n',\n",
              "    '    messages = state[\"messages\"]\\n',\n",
              "    '    question = messages[0].content\\n',\n",
              "    '\\n',\n",
              "    '    prompt = f\"Vous êtes un expert en reformulation de questions. Votre but est d\\'améliorer la question de l\\'utilisateur pour la rendre plus spécifique et plus claire pour un moteur de recherche. Ne répondez pas à la question, reformulez-la simplement.\\\\n\\\\nQuestion originale : {question}\\\\n\\\\nQuestion améliorée :\"\\n',\n",
              "    '    \\n',\n",
              "    '    response = llm.invoke(prompt)\\n',\n",
              "    '    # Nous remplaçons la question originale par la nouvelle pour relancer le cycle.\\n',\n",
              "    '    new_question = HumanMessage(content=response.content)\\n',\n",
              "    '    return {\"messages\": [new_question]}\\n',\n",
              "    '\\n',\n",
              "    'print(\"Tous les nœuds et fonctions du graphe sont définis.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['### Étape 5 : Assemblage du Graphe avec LangGraph\\n',\n",
              "    '\\n',\n",
              "    \"Maintenant que nous avons tous nos composants (état, nœuds), il est temps de les assembler pour former notre agent. C'est ici que la magic de LangGraph opère.\\n\",\n",
              "    '\\n',\n",
              "    '1.  **Instanciation du `StateGraph`** : Nous créons une instance de notre graphe en lui passant la structure de notre état, `AgentState`.\\n',\n",
              "    '2.  **Ajout des Nœuds** : Nous déclarons chaque fonction que nous avons définie comme un nœud dans notre graphe, en lui donnant un nom unique.\\n',\n",
              "    \"3.  **Définition du Point d'Entrée** : Nous indiquons au graphe par quel nœud le processus doit commencer (`set_entry_point`).\\n\",\n",
              "    \"4.  **Création des Arêtes Conditionnelles** : C'est la partie la plus importante. Nous connectons les nœuds en définissant des règles. \\n\",\n",
              "    \"    -   Après le nœud `ai_assistant`, nous utilisons une condition pour vérifier si le LLM a décidé d'appeler un outil. Si oui, on va au nœud `retrieve`. Sinon, le travail est terminé (`END`).\\n\",\n",
              "    '    -   Après le nœud `retrieve`, nous utilisons notre fonction `grade_documents` comme aiguilleur. En fonction de sa sortie (\"generate\" ou \"rewrite\"), le flux est dirigé vers le nœud correspondant.\\n',\n",
              "    '5.  **Création des Arêtes Normales** : Nous ajoutons les connexions simples.\\n',\n",
              "    \"    -   Après la réécriture (`rewrite`), on boucle en retournant à l'assistant (`ai_assistant`) pour tenter une nouvelle recherche avec la question améliorée. **C'est ce qui crée notre boucle d'auto-correction.**\\n\",\n",
              "    '    -   Après la génération (`generate`), le processus est terminé (`END`).']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['from langgraph.graph import StateGraph, END\\n',\n",
              "    '\\n',\n",
              "    '# 1. Instanciation du StateGraph\\n',\n",
              "    'workflow = StateGraph(AgentState)\\n',\n",
              "    '\\n',\n",
              "    '# 2. Ajout des Nœuds\\n',\n",
              "    'workflow.add_node(\"My_AI_Assistant\", ai_assistant)\\n',\n",
              "    'workflow.add_node(\"retrieve\", retrieve)\\n',\n",
              "    'workflow.add_node(\"rewrite\", rewrite)\\n',\n",
              "    'workflow.add_node(\"generate\", generate)\\n',\n",
              "    '\\n',\n",
              "    \"# 3. Définition du Point d'Entrée\\n\",\n",
              "    'workflow.set_entry_point(\"My_AI_Assistant\")\\n',\n",
              "    '\\n',\n",
              "    '# 4. Création des Arêtes Conditionnelles\\n',\n",
              "    '\\n',\n",
              "    '# Condition pour décider si on doit utiliser un outil ou non\\n',\n",
              "    'def should_retrieve(state: AgentState):\\n',\n",
              "    '    print(\"---ROUTAGE : Outil ou Fin ?---\")\\n',\n",
              "    '    messages = state[\"messages\"]\\n',\n",
              "    '    last_message = messages[-1]\\n',\n",
              "    \"    # Si le dernier message contient des appels d'outils, alors on doit agir.\\n\",\n",
              "    '    if last_message.tool_calls:\\n',\n",
              "    '        print(\"Décision : Appel d\\'outil détecté. Passage à la recherche.\")\\n',\n",
              "    '        return \"retrieve\"\\n',\n",
              "    \"    # Sinon, le LLM a répondu directement, c'est la fin.\\n\",\n",
              "    '    print(\"Décision : Pas d\\'appel d\\'outil. Fin du processus.\")\\n',\n",
              "    '    return \"end\"\\n',\n",
              "    '\\n',\n",
              "    'workflow.add_conditional_edges(\\n',\n",
              "    '    \"My_AI_Assistant\",\\n',\n",
              "    '    should_retrieve,\\n',\n",
              "    '    {\\n',\n",
              "    '        \"retrieve\": \"retrieve\",\\n',\n",
              "    '        \"end\": END,\\n',\n",
              "    '    },\\n',\n",
              "    ')\\n',\n",
              "    '\\n',\n",
              "    '# Condition pour décider si on génère une réponse ou si on réécrit la question\\n',\n",
              "    'workflow.add_conditional_edges(\\n',\n",
              "    '    \"retrieve\",\\n',\n",
              "    '    grade_documents,\\n',\n",
              "    '    {\\n',\n",
              "    '        \"generate\": \"generate\",\\n',\n",
              "    '        \"rewrite\": \"rewrite\",\\n',\n",
              "    '    },\\n',\n",
              "    ')\\n',\n",
              "    '\\n',\n",
              "    '# 5. Création des Arêtes Normales\\n',\n",
              "    '\\n',\n",
              "    \"# Après la réécriture, on boucle en retournant à l'assistant\\n\",\n",
              "    'workflow.add_edge(\"rewrite\", \"My_AI_Assistant\")\\n',\n",
              "    '\\n',\n",
              "    \"# Après la génération, c'est la fin\\n\",\n",
              "    'workflow.add_edge(\"generate\", END)\\n',\n",
              "    '\\n',\n",
              "    'print(\"Graphe assemblé.\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': ['### Étape 6 : Compilation et Test\\n',\n",
              "    '\\n',\n",
              "    'Notre graphe est maintenant entièrement défini. La dernière étape de construction est de le **compiler**. La compilation transforme notre définition de graphe en un objet `Runnable` que nous pouvons appeler.\\n',\n",
              "    '\\n',\n",
              "    'Nous allons ensuite le tester avec différentes questions pour observer son comportement :\\n',\n",
              "    '-   Une salutation simple (devrait finir directement).\\n',\n",
              "    '-   Une question précise (devrait trouver des documents et générer une réponse).\\n',\n",
              "    '-   Une question vague (devrait déclencher la boucle de réécriture).\\n',\n",
              "    \"-   Une question hors sujet (devrait échouer à trouver des documents et potentiellement boucler ou s'arrêter).\"]},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# Compilation du graphe\\n',\n",
              "    'app = workflow.compile()\\n',\n",
              "    '\\n',\n",
              "    '# Visualisation du graphe (nécessite graphviz)\\n',\n",
              "    \"# C'est très utile pour vérifier que nos connexions sont correctes.\\n\",\n",
              "    'try:\\n',\n",
              "    '    from IPython.display import Image, display\\n',\n",
              "    '    display(Image(app.get_graph().draw_png()))\\n',\n",
              "    'except ImportError:\\n',\n",
              "    '    print(\"Graphviz non installé. Impossible d\\'afficher le graphe.\")\\n',\n",
              "    '\\n',\n",
              "    'print(\"Graphe compilé et prêt.\")']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['# --- Tests --- \\n',\n",
              "    '\\n',\n",
              "    'import time\\n',\n",
              "    '\\n',\n",
              "    'def run_test(question):\\n',\n",
              "    '    print(f\"\\\\n--- TEST AVEC LA QUESTION : \\'{question}\\' ---\")\\n',\n",
              "    '    inputs = {\"messages\": [HumanMessage(content=question)]}\\n',\n",
              "    '    final_state = app.invoke(inputs)\\n',\n",
              "    \"    final_response = final_state['messages'][-1]\\n\",\n",
              "    '    print(\"\\\\n--- RÉPONSE FINALE ---\")\\n',\n",
              "    '    print(final_response.content)\\n',\n",
              "    '    print(\"-------------------------\")\\n',\n",
              "    '\\n',\n",
              "    '# Test 1: Salutation simple\\n',\n",
              "    '# run_test(\"Bonjour, comment ça va ?\")\\n',\n",
              "    '\\n',\n",
              "    '# Test 2: Question précise\\n',\n",
              "    '# run_test(\"Qu\\'est-ce que le RAG auto-correctif ?\")\\n',\n",
              "    '\\n',\n",
              "    '# Test 3: Question vague\\n',\n",
              "    '# run_test(\"Parle-moi des agents.\")\\n',\n",
              "    '\\n',\n",
              "    '# Test 4: Question hors-sujet\\n',\n",
              "    '# run_test(\"Quelle est la meilleure recette de crêpes ?\")']},\n",
              "  {'cell_type': 'markdown',\n",
              "   'metadata': {},\n",
              "   'source': [\"### Étape 7 : Fonction d'Interface pour Streamlit\\n\",\n",
              "    '\\n',\n",
              "    'Pour que notre application Streamlit puisse utiliser la logique que nous venons de construire, nous devons créer une fonction simple qui sert de pont. \\n',\n",
              "    '\\n',\n",
              "    'Cette fonction, `run_agent`, prendra une chaîne de caractères (la question de l\\'utilisateur) en entrée et retournera la réponse finale de l\\'agent. Pour une meilleure expérience utilisateur dans Streamlit, nous allons la transformer en **générateur**. Elle produira la réponse morceau par morceau (`stream`), ce qui permettra d\\'afficher la réponse au fur et à mesure qu\\'elle est générée, donnant une impression de \"live typing\".']},\n",
              "  {'cell_type': 'code',\n",
              "   'execution_count': None,\n",
              "   'metadata': {},\n",
              "   'outputs': [],\n",
              "   'source': ['import time\\n',\n",
              "    '\\n',\n",
              "    'def run_agent(user_query: str):\\n',\n",
              "    '    \"\"\"\\n',\n",
              "    \"    Exécute le graphe de l'agent pour une requête utilisateur donnée et streame la réponse.\\n\",\n",
              "    '    \\n',\n",
              "    '    Args:\\n',\n",
              "    \"        user_query: La question posée par l'utilisateur.\\n\",\n",
              "    '        \\n',\n",
              "    '    Yields:\\n',\n",
              "    \"        str: Des morceaux de la réponse finale de l'agent.\\n\",\n",
              "    '    \"\"\"\\n',\n",
              "    '    inputs = {\"messages\": [HumanMessage(content=user_query)]}\\n',\n",
              "    '    \\n',\n",
              "    '    # Utiliser .stream() au lieu de .invoke() pour obtenir un générateur\\n',\n",
              "    \"    # Cela nous permet de traiter les événements au fur et à mesure qu'ils se produisent dans le graphe.\\n\",\n",
              "    '    full_response = \"\"\\n',\n",
              "    '    \\n',\n",
              "    '    # Le stream retourne les états intermédiaires du graphe.\\n',\n",
              "    '    # Nous nous intéressons au dernier message du dernier état.\\n',\n",
              "    '    for output in app.stream(inputs):\\n',\n",
              "    \"        # La clé du dictionnaire correspond au nom du noeud qui vient de s'exécuter\\n\",\n",
              "    '        for key, value in output.items():\\n',\n",
              "    '            if key == \"__end__\": # La fin du graphe\\n',\n",
              "    '                break\\n',\n",
              "    '            # Nous pouvons ajouter des logs ici pour voir la progression\\n',\n",
              "    '            # print(f\"Output from node \\'{key}\\': {value}\")\\n',\n",
              "    '            pass\\n',\n",
              "    '            \\n',\n",
              "    \"    # Une fois le stream terminé, l'état final est dans la dernière valeur\\n\",\n",
              "    '    final_state = value\\n',\n",
              "    \"    if final_state and 'messages' in final_state and final_state['messages']:\\n\",\n",
              "    \"        final_response_message = final_state['messages'][-1]\\n\",\n",
              "    '        full_response = final_response_message.content\\n',\n",
              "    '    else:\\n',\n",
              "    '        full_response = \"Désolé, je n\\'ai pas pu trouver de réponse.\"\\n',\n",
              "    '\\n',\n",
              "    \"    # Simuler un streaming de la réponse finale pour l'affichage\\n\",\n",
              "    '    for char in full_response:\\n',\n",
              "    '        yield char\\n',\n",
              "    '        time.sleep(0.01) # Petite pause pour un effet de frappe naturel\\n',\n",
              "    '\\n',\n",
              "    'print(\"La fonction \\'run_agent\\' est prête à être appelée par Streamlit.\")\\n',\n",
              "    '\\n',\n",
              "    \"# Exemple d'utilisation de la fonction run_agent\\n\",\n",
              "    '# print(\"\\\\n--- TEST DE LA FONCTION STREAMLIT ---\")\\n',\n",
              "    '# query = \"Explique le concept de \\'plan of thought\\' pour les agents IA.\"\\n',\n",
              "    '# response_generator = run_agent(query)\\n',\n",
              "    '# for chunk in response_generator:\\n',\n",
              "    '#     print(chunk, end=\"\", flush=True)\\n',\n",
              "    '# print()']}],\n",
              " 'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
              "   'language': 'python',\n",
              "   'name': 'python3'},\n",
              "  'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
              "   'file_extension': '.py',\n",
              "   'mimetype': 'text/x-python',\n",
              "   'name': 'python',\n",
              "   'nbconvert_exporter': 'python',\n",
              "   'pygments_lexer': 'ipython3',\n",
              "   'version': '3.10.9'}},\n",
              " 'nbformat': 4,\n",
              " 'nbformat_minor': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "{\n",
        " \"cells\": [\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"# Challenge : Construire un Agent RAG Auto-Correcteur avec LangGraph\\n\",\n",
        "    \"\\n\",\n",
        "    \"Bienvenue dans ce notebook ! L'objectif est de construire un agent conversationnel sophistiqué en utilisant le framework **LangGraph**. \\n\",\n",
        "    \"\\n\",\n",
        "    \"Cet agent ne se contentera pas de répondre à des questions sur la base de documents fournis (le principe du RAG - Retrieval-Augmented Generation), mais il sera capable d'évaluer la pertinence des informations récupérées. Si les informations ne sont pas pertinentes, l'agent **reformulera la question** de l'utilisateur pour tenter d'obtenir de meilleurs résultats, créant ainsi une boucle de raisonnement et d'auto-correction.\\n\",\n",
        "    \"\\n\",\n",
        "    \"## Plan du Notebook\\n\",\n",
        "    \"\\n\",\n",
        "    \"1.  **Configuration de l'Environnement** : Chargement des clés API et des variables d'environnement.\\n\",\n",
        "    \"2.  **Construction de la Base de Connaissances (Le \\\"R\\\" de RAG)** : Chargement de documents depuis le web, découpage, et création d'un index vectoriel avec ChromaDB.\\n\",\n",
        "    \"3.  **Définition de l'État de l'Agent** : Création de la structure de données qui servira de \\\"mémoire\\\" à notre agent tout au long du processus.\\n\",\n",
        "    \"4.  **Création des Outils et des Nœuds du Graphe** : Définition des différentes fonctions qui représenteront les étapes de notre agent (recherche, évaluation, réécriture, génération).\\n\",\n",
        "    \"5.  **Assemblage du Graphe avec LangGraph** : Connexion des nœuds avec une logique conditionnelle pour orchestrer le flux de travail de l'agent.\\n\",\n",
        "    \"6.  **Compilation et Test** : Création de l'application exécutable et validation de son comportement.\\n\",\n",
        "    \"7.  **Fonction d'Interface pour Streamlit** : Création d'une fonction `run_agent` simple que notre application Streamlit pourra appeler.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"### Étape 1 : Configuration de l'Environnement\\n\",\n",
        "    \"\\n\",\n",
        "    \"La première étape consiste à s'assurer que notre environnement est correctement configuré. Nous allons charger les bibliothèques nécessaires et les clés API à partir d'un fichier `.env`. C'est une pratique essentielle pour garder nos informations sensibles (comme les clés API) séparées de notre code.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"import os\\n\",\n",
        "    \"from dotenv import load_dotenv\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Charger les variables d'environnement du fichier .env\\n\",\n",
        "    \"# Cela permet à os.getenv() de trouver les clés que nous avons stockées.\\n\",\n",
        "    \"load_dotenv()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Configuration optionnelle pour le traçage avec LangSmith\\n\",\n",
        "    \"# LangSmith est un outil fantastique pour visualiser et déboguer les chaînes et agents LangChain.\\n\",\n",
        "    \"# C'est très utile pour comprendre ce qui se passe sous le capot de notre graphe.\\n\",\n",
        "    \"os.environ[\\\"LANGCHAIN_TRACING_V2\\\"] = \\\"true\\\"\\n\",\n",
        "    \"os.environ[\\\"LANGCHAIN_ENDPOINT\\\"] = \\\"https://api.smith.langchain.com\\\"\\n\",\n",
        "    \"# Assurez-vous que LANGCHAIN_API_KEY est défini dans votre fichier .env si vous souhaitez utiliser LangSmith\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Récupération des clés API\\n\",\n",
        "    \"GROQ_API_KEY = os.getenv(\\\"GROQ_API_KEY\\\")\\n\",\n",
        "    \"GOOGLE_API_KEY = os.getenv(\\\"GOOGLE_API_KEY\\\") # Utilisé pour les embeddings\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"Clés API chargées.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"### Étape 2 : Construction de la Base de Connaissances (Le \\\"R\\\" de RAG)\\n\",\n",
        "    \"\\n\",\n",
        "    \"Un agent RAG a besoin d'une base de connaissances sur laquelle s'appuyer. Nous allons construire cette base en plusieurs sous-étapes :\\n\",\n",
        "    \"\\n\",\n",
        "    \"1.  **Chargement des données** : Nous utiliserons `WebBaseLoader` de LangChain pour charger le contenu de quelques articles de blog pertinents sur le Machine Learning et les agents.\\n\",\n",
        "    \"2.  **Découpage des documents** : Les documents bruts sont souvent trop longs pour être traités efficacement par un LLM. Nous les découperons en plus petits morceaux (chunks) avec `RecursiveCharacterTextSplitter`. Cette méthode est robuste car elle essaie de couper le texte sur des séparateurs logiques (paragraphes, phrases, etc.).\\n\",\n",
        "    \"3.  **Création des Embeddings** : Pour que la machine comprenne le sens sémantique de nos morceaux de texte, nous devons les convertir en vecteurs numériques (embeddings). Nous utiliserons un modèle d'embedding de Google.\\n\",\n",
        "    \"4.  **Stockage dans un Vectorstore** : Ces vecteurs seront stockés et indexés dans une base de données vectorielle, **ChromaDB**. Cela nous permettra d'effectuer des recherches de similarité rapides : trouver les morceaux de texte les plus pertinents pour une question donnée.\\n\",\n",
        "    \"5.  **Création de l'outil de recherche (Retriever Tool)** : Enfin, nous encapsulerons notre logique de recherche dans un `Tool` LangChain. Cela permettra à notre agent d'appeler la recherche dans la base de connaissances comme n'importe quel autre outil.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"from langchain_community.document_loaders import WebBaseLoader\\n\",\n",
        "    \"from langchain.text_splitter import RecursiveCharacterTextSplitter\\n\",\n",
        "    \"from langchain_community.vectorstores import Chroma\\n\",\n",
        "    \"from langchain_google_genai import GoogleGenerativeAIEmbeddings\\n\",\n",
        "    \"from langchain.tools.retriever import create_retriever_tool\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 1. Chargement des données\\n\",\n",
        "    \"# Nous choisissons des URLs qui parlent des agents et du RAG pour que notre agent soit bien informé sur ce sujet.\\n\",\n",
        "    \"urls = [\\n\",\n",
        "    \"    \\\"https://lilianweng.github.io/posts/2023-06-23-agent/\\\",\\n\",\n",
        "    \"    \\\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\\\",\\n\",\n",
        "    \"    \\\"https://lilianweng.github.io/posts/2023-10-25-adv-rag/\\\",\\n\",\n",
        "    \"]\\n\",\n",
        "    \"\\n\",\n",
        "    \"loader = WebBaseLoader(urls)\\n\",\n",
        "    \"docs = loader.load()\\n\",\n",
        "    \"print(f\\\"{len(docs)} documents chargés.\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 2. Découpage des documents\\n\",\n",
        "    \"# chunk_size définit la taille maximale de chaque morceau.\\n\",\n",
        "    \"# chunk_overlap conserve une petite partie de la fin d'un morceau au début du suivant pour préserver le contexte.\\n\",\n",
        "    \"text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\\n\",\n",
        "    \"splits = text_splitter.split_documents(docs)\\n\",\n",
        "    \"print(f\\\"{len(splits)} morceaux créés.\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 3. Création des Embeddings\\n\",\n",
        "    \"# Nous utilisons un modèle de Google. Il est performant et bien intégré avec LangChain.\\n\",\n",
        "    \"embeddings = GoogleGenerativeAIEmbeddings(model=\\\"models/embedding-001\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 4. Stockage dans un Vectorstore\\n\",\n",
        "    \"# Chroma est une base de données vectorielle open-source et légère, parfaite pour des projets comme celui-ci.\\n\",\n",
        "    \"# \\\"from_documents\\\" s'occupe de générer les embeddings et de les stocker.\\n\",\n",
        "    \"vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 5. Création de l'outil de recherche\\n\",\n",
        "    \"# Nous transformons notre vectorstore en un \\\"retriever\\\", qui est un objet capable de récupérer des documents.\\n\",\n",
        "    \"retriever = vectorstore.as_retriever()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Enfin, nous créons un \\\"Tool\\\". L'agent utilisera le nom et la description pour décider quand utiliser cet outil.\\n\",\n",
        "    \"retriever_tool = create_retriever_tool(\\n\",\n",
        "    \"    retriever,\\n\",\n",
        "    \"    name=\\\"retrieval_tool\\\",\\n\",\n",
        "    \"    description=\\\"Recherche des informations sur les agents d'IA, le RAG et le prompt engineering. Utilise cet outil pour répondre aux questions sur le machine learning.\\\"\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"tools = [retriever_tool]\\n\",\n",
        "    \"print(\\\"Outil de recherche créé et prêt à l'emploi.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"### Étape 3 : Définition de l'État de l'Agent\\n\",\n",
        "    \"\\n\",\n",
        "    \"LangGraph fonctionne comme une **machine à états**. Nous devons définir explicitement la structure des données qui persisteront et seront modifiées à travers les différents nœuds de notre graphe. C'est ce que nous appelons l'\\\"État\\\" (`State`).\\n\",\n",
        "    \"\\n\",\n",
        "    \"Notre état, que nous nommerons `AgentState`, contiendra l'historique de la conversation. Nous utilisons `TypedDict` pour définir clairement les champs et leurs types. `Annotated[list[BaseMessage], operator.add]` est une syntaxe spéciale de LangGraph qui indique que le champ `messages` est une liste de messages et que les nouvelles valeurs doivent être ajoutées à la liste existante, plutôt que de la remplacer.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"from typing import TypedDict, Annotated, List\\n\",\n",
        "    \"from langchain_core.messages import BaseMessage\\n\",\n",
        "    \"import operator\\n\",\n",
        "    \"\\n\",\n",
        "    \"class AgentState(TypedDict):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    Définit la structure de l'état de notre agent.\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    Attributes:\\n\",\n",
        "    \"        messages: L'historique des messages de la conversation.\\n\",\n",
        "    \"                  L'annotation `operator.add` signifie que les nouveaux messages\\n\",\n",
        "    \"                  seront ajoutés à la liste existante à chaque étape.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    messages: Annotated[list[BaseMessage], operator.add]\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"État de l'agent défini.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"### Étape 4 : Création des Outils et des Nœuds du Graphe\\n\",\n",
        "    \"\\n\",\n",
        "    \"C'est le cœur de notre agent. Nous allons définir plusieurs fonctions Python qui serviront de **nœuds** dans notre graphe. Chaque nœud effectue une tâche spécifique.\\n\",\n",
        "    \"\\n\",\n",
        "    \"1.  **Modèle LLM** : Nous choisissons le modèle qui servira de \\\"cerveau\\\" à notre agent. Nous utilisons `ChatGroq` pour sa rapidité.\\n\",\n",
        "    \"2.  **Nœud `My_AI_Assistant` (Routeur)** : C'est le point d'entrée. Ce nœud examine la dernière question de l'utilisateur et, en utilisant le LLM et la description des outils disponibles, décide s'il doit appeler un outil (notre `retrieval_tool`) ou s'il peut répondre directement.\\n\",\n",
        "    \"3.  **Nœud `retrieve` (Action)** : Ce nœud est un `ToolNode`. Il est responsable de l'exécution effective des appels d'outils décidés par le routeur.\\n\",\n",
        "    \"4.  **Fonction `grade_documents` (Aiguilleur Conditionnel)** : Ce n'est pas un nœud, mais une fonction qui sera utilisée pour créer une **arête conditionnelle**. Elle prend les documents récupérés, demande à un LLM de les noter (sont-ils pertinents ?), et retourne une décision (\\\"generate\\\" ou \\\"rewrite\\\"). Pour garantir une sortie fiable, nous utilisons les **sorties structurées** de LangChain, forçant le LLM à répondre dans un format JSON que nous définissons.\\n\",\n",
        "    \"5.  **Nœud `generate` (Générateur de réponse)** : Ce nœud prend les documents jugés pertinents et la question de l'utilisateur, et génère la réponse finale.\\n\",\n",
        "    \"6.  **Nœud `rewrite` (Réécrivain de question)** : Si les documents ne sont pas pertinents, ce nœud est appelé. Il prend la question originale et demande au LLM de la reformuler pour qu'elle soit plus claire ou plus spécifique, dans l'espoir d'obtenir de meilleurs résultats de recherche.\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"from langchain_groq import ChatGroq\\n\",\n",
        "    \"from langgraph.prebuilt import ToolNode\\n\",\n",
        "    \"from langchain_core.pydantic_v1 import BaseModel, Field\\n\",\n",
        "    \"from langchain_core.output_parsers import JsonOutputParser\\n\",\n",
        "    \"from langchain_core.prompts import PromptTemplate\\n\",\n",
        "    \"from langchain_core.messages import HumanMessage, ToolMessage\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 1. Modèle LLM\\n\",\n",
        "    \"# Nous utilisons Groq pour sa vitesse d'inférence impressionnante.\\n\",\n",
        "    \"# Le modèle Llama3 8b est un bon compromis entre performance et rapidité.\\n\",\n",
        "    \"llm = ChatGroq(model=\\\"llama3-8b-8192\\\", temperature=0)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Lier les outils au LLM. Cela permet au LLM de savoir quels outils il peut appeler.\\n\",\n",
        "    \"llm_with_tools = llm.bind_tools(tools)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 2. Nœud `My_AI_Assistant` (Routeur)\\n\",\n",
        "    \"def ai_assistant(state: AgentState):\\n\",\n",
        "    \"    \\\"\\\"\\\"Point d'entrée : décide s'il faut appeler un outil ou non.\\\"\\\"\\\"\\n\",\n",
        "    \"    print(\\\"---ASSISTANT (ROUTEUR)---\\\")\\n\",\n",
        "    \"    messages = state[\\\"messages\\\"]\\n\",\n",
        "    \"    # Appel au LLM avec la capacité d'utiliser des outils\\n\",\n",
        "    \"    response = llm_with_tools.invoke(messages)\\n\",\n",
        "    \"    # La réponse est ajoutée à l'état, qu'elle contienne un appel d'outil ou une réponse directe.\\n\",\n",
        "    \"    return {\\\"messages\\\": [response]}\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 3. Nœud `retrieve` (Action)\\n\",\n",
        "    \"# ToolNode est un nœud pré-construit qui exécute les outils appelés par le LLM.\\n\",\n",
        "    \"retrieve = ToolNode(tools)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 4. Fonction `grade_documents` (Aiguilleur Conditionnel)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Définition du format de sortie pour l'évaluation\\n\",\n",
        "    \"class Grade(BaseModel):\\n\",\n",
        "    \"    \\\"\\\"\\\"Évaluation binaire de la pertinence des documents.\\\"\\\"\\\"\\n\",\n",
        "    \"    binary_score: str = Field(description=\\\"Les documents sont-ils pertinents ? 'oui' ou 'non'\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"def grade_documents(state: AgentState):\\n\",\n",
        "    \"    \\\"\\\"\\\"Évalue si les documents récupérés sont pertinents pour la question.\\\"\\\"\\\"\\n\",\n",
        "    \"    print(\\\"---ÉVALUATION DES DOCUMENTS---\\\")\\n\",\n",
        "    \"    messages = state[\\\"messages\\\"]\\n\",\n",
        "    \"    last_message = messages[-1]\\n\",\n",
        "    \"    question = messages[0].content\\n\",\n",
        "    \"\\n\",\n",
        "    \"    # Créer un parser pour la sortie structurée\\n\",\n",
        "    \"    parser = JsonOutputParser(pydantic_object=Grade)\\n\",\n",
        "    \"\\n\",\n",
        "    \"    # Créer un LLM avec la sortie structurée\\n\",\n",
        "    \"    structured_llm_grader = llm.with_structured_output(Grade)\\n\",\n",
        "    \"\\n\",\n",
        "    \"    # Prompt pour l'évaluation\\n\",\n",
        "    \"    prompt = PromptTemplate(\\n\",\n",
        "    \"        template=\\\"Vous êtes un évaluateur qui note la pertinence des documents récupérés par rapport à une question utilisateur.\\\\n\\\"\\n\",\n",
        "    \"                 \\\"Voici les documents récupérés : \\\\n\\\\n {documents} \\\\n\\\\n\\\"\\n\",\n",
        "    \"                 \\\"Voici la question de l'utilisateur : {question} \\\\n\\\"\\n\",\n",
        "    \"                 \\\"Si les documents contiennent des mots-clés ou des concepts sémantiques liés à la question, notez-les comme pertinents.\\\\n\\\"\\n\",\n",
        "    \"                 \\\"Donnez une note binaire, 'oui' ou 'non', pour indiquer si les documents sont pertinents.\\\",\\n\",\n",
        "    \"        input_variables=[\\\"question\\\", \\\"documents\\\"],\\n\",\n",
        "    \"    )\\n\",\n",
        "    \"\\n\",\n",
        "    \"    chain = prompt | structured_llm_grader\\n\",\n",
        "    \"    docs = last_message.content\\n\",\n",
        "    \"    response = chain.invoke({\\\"question\\\": question, \\\"documents\\\": docs})\\n\",\n",
        "    \"\\n\",\n",
        "    \"    if response.binary_score == \\\"oui\\\":\\n\",\n",
        "    \"        print(\\\"Décision : Documents pertinents. Passage à la génération.\\\")\\n\",\n",
        "    \"        return \\\"generate\\\"\\n\",\n",
        "    \"    else:\\n\",\n",
        "    \"        print(\\\"Décision : Documents non pertinents. Passage à la réécriture.\\\")\\n\",\n",
        "    \"        return \\\"rewrite\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 5. Nœud `generate` (Générateur de réponse)\\n\",\n",
        "    \"def generate(state: AgentState):\\n\",\n",
        "    \"    \\\"\\\"\\\"Génère une réponse finale en utilisant les documents et la question.\\\"\\\"\\\"\\n\",\n",
        "    \"    print(\\\"---GÉNÉRATION DE LA RÉPONSE---\\\")\\n\",\n",
        "    \"    messages = state[\\\"messages\\\"]\\n\",\n",
        "    \"    question = messages[0].content\\n\",\n",
        "    \"    last_message = messages[-1]\\n\",\n",
        "    \"    docs = last_message.content\\n\",\n",
        "    \"\\n\",\n",
        "    \"    prompt = f\\\"Vous êtes un assistant IA spécialisé dans le Machine Learning. Répondez à la question de l'utilisateur en vous basant sur le contexte suivant :\\\\n\\\\nContexte : {docs}\\\\n\\\\nQuestion : {question}\\\\n\\\\nRéponse :\\\"\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    response = llm.invoke(prompt)\\n\",\n",
        "    \"    return {\\\"messages\\\": [response]}\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 6. Nœud `rewrite` (Réécrivain de question)\\n\",\n",
        "    \"def rewrite(state: AgentState):\\n\",\n",
        "    \"    \\\"\\\"\\\"Reformule la question de l'utilisateur pour une meilleure recherche.\\\"\\\"\\\"\\n\",\n",
        "    \"    print(\\\"---RÉÉCRITURE DE LA QUESTION---\\\")\\n\",\n",
        "    \"    messages = state[\\\"messages\\\"]\\n\",\n",
        "    \"    question = messages[0].content\\n\",\n",
        "    \"\\n\",\n",
        "    \"    prompt = f\\\"Vous êtes un expert en reformulation de questions. Votre but est d'améliorer la question de l'utilisateur pour la rendre plus spécifique et plus claire pour un moteur de recherche. Ne répondez pas à la question, reformulez-la simplement.\\\\n\\\\nQuestion originale : {question}\\\\n\\\\nQuestion améliorée :\\\"\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    response = llm.invoke(prompt)\\n\",\n",
        "    \"    # Nous remplaçons la question originale par la nouvelle pour relancer le cycle.\\n\",\n",
        "    \"    new_question = HumanMessage(content=response.content)\\n\",\n",
        "    \"    return {\\\"messages\\\": [new_question]}\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"Tous les nœuds et fonctions du graphe sont définis.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"### Étape 5 : Assemblage du Graphe avec LangGraph\\n\",\n",
        "    \"\\n\",\n",
        "    \"Maintenant que nous avons tous nos composants (état, nœuds), il est temps de les assembler pour former notre agent. C'est ici que la magic de LangGraph opère.\\n\",\n",
        "    \"\\n\",\n",
        "    \"1.  **Instanciation du `StateGraph`** : Nous créons une instance de notre graphe en lui passant la structure de notre état, `AgentState`.\\n\",\n",
        "    \"2.  **Ajout des Nœuds** : Nous déclarons chaque fonction que nous avons définie comme un nœud dans notre graphe, en lui donnant un nom unique.\\n\",\n",
        "    \"3.  **Définition du Point d'Entrée** : Nous indiquons au graphe par quel nœud le processus doit commencer (`set_entry_point`).\\n\",\n",
        "    \"4.  **Création des Arêtes Conditionnelles** : C'est la partie la plus importante. Nous connectons les nœuds en définissant des règles. \\n\",\n",
        "    \"    -   Après le nœud `ai_assistant`, nous utilisons une condition pour vérifier si le LLM a décidé d'appeler un outil. Si oui, on va au nœud `retrieve`. Sinon, le travail est terminé (`END`).\\n\",\n",
        "    \"    -   Après le nœud `retrieve`, nous utilisons notre fonction `grade_documents` comme aiguilleur. En fonction de sa sortie (\\\"generate\\\" ou \\\"rewrite\\\"), le flux est dirigé vers le nœud correspondant.\\n\",\n",
        "    \"5.  **Création des Arêtes Normales** : Nous ajoutons les connexions simples.\\n\",\n",
        "    \"    -   Après la réécriture (`rewrite`), on boucle en retournant à l'assistant (`ai_assistant`) pour tenter une nouvelle recherche avec la question améliorée. **C'est ce qui crée notre boucle d'auto-correction.**\\n\",\n",
        "    \"    -   Après la génération (`generate`), le processus est terminé (`END`).\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"from langgraph.graph import StateGraph, END\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 1. Instanciation du StateGraph\\n\",\n",
        "    \"workflow = StateGraph(AgentState)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 2. Ajout des Nœuds\\n\",\n",
        "    \"workflow.add_node(\\\"My_AI_Assistant\\\", ai_assistant)\\n\",\n",
        "    \"workflow.add_node(\\\"retrieve\\\", retrieve)\\n\",\n",
        "    \"workflow.add_node(\\\"rewrite\\\", rewrite)\\n\",\n",
        "    \"workflow.add_node(\\\"generate\\\", generate)\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 3. Définition du Point d'Entrée\\n\",\n",
        "    \"workflow.set_entry_point(\\\"My_AI_Assistant\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 4. Création des Arêtes Conditionnelles\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Condition pour décider si on doit utiliser un outil ou non\\n\",\n",
        "    \"def should_retrieve(state: AgentState):\\n\",\n",
        "    \"    print(\\\"---ROUTAGE : Outil ou Fin ?---\\\")\\n\",\n",
        "    \"    messages = state[\\\"messages\\\"]\\n\",\n",
        "    \"    last_message = messages[-1]\\n\",\n",
        "    \"    # Si le dernier message contient des appels d'outils, alors on doit agir.\\n\",\n",
        "    \"    if last_message.tool_calls:\\n\",\n",
        "    \"        print(\\\"Décision : Appel d'outil détecté. Passage à la recherche.\\\")\\n\",\n",
        "    \"        return \\\"retrieve\\\"\\n\",\n",
        "    \"    # Sinon, le LLM a répondu directement, c'est la fin.\\n\",\n",
        "    \"    print(\\\"Décision : Pas d'appel d'outil. Fin du processus.\\\")\\n\",\n",
        "    \"    return \\\"end\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"workflow.add_conditional_edges(\\n\",\n",
        "    \"    \\\"My_AI_Assistant\\\",\\n\",\n",
        "    \"    should_retrieve,\\n\",\n",
        "    \"    {\\n\",\n",
        "    \"        \\\"retrieve\\\": \\\"retrieve\\\",\\n\",\n",
        "    \"        \\\"end\\\": END,\\n\",\n",
        "    \"    },\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Condition pour décider si on génère une réponse ou si on réécrit la question\\n\",\n",
        "    \"workflow.add_conditional_edges(\\n\",\n",
        "    \"    \\\"retrieve\\\",\\n\",\n",
        "    \"    grade_documents,\\n\",\n",
        "    \"    {\\n\",\n",
        "    \"        \\\"generate\\\": \\\"generate\\\",\\n\",\n",
        "    \"        \\\"rewrite\\\": \\\"rewrite\\\",\\n\",\n",
        "    \"    },\\n\",\n",
        "    \")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# 5. Création des Arêtes Normales\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Après la réécriture, on boucle en retournant à l'assistant\\n\",\n",
        "    \"workflow.add_edge(\\\"rewrite\\\", \\\"My_AI_Assistant\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Après la génération, c'est la fin\\n\",\n",
        "    \"workflow.add_edge(\\\"generate\\\", END)\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"Graphe assemblé.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"### Étape 6 : Compilation et Test\\n\",\n",
        "    \"\\n\",\n",
        "    \"Notre graphe est maintenant entièrement défini. La dernière étape de construction est de le **compiler**. La compilation transforme notre définition de graphe en un objet `Runnable` que nous pouvons appeler.\\n\",\n",
        "    \"\\n\",\n",
        "    \"Nous allons ensuite le tester avec différentes questions pour observer son comportement :\\n\",\n",
        "    \"-   Une salutation simple (devrait finir directement).\\n\",\n",
        "    \"-   Une question précise (devrait trouver des documents et générer une réponse).\\n\",\n",
        "    \"-   Une question vague (devrait déclencher la boucle de réécriture).\\n\",\n",
        "    \"-   Une question hors sujet (devrait échouer à trouver des documents et potentiellement boucler ou s'arrêter).\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# Compilation du graphe\\n\",\n",
        "    \"app = workflow.compile()\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Visualisation du graphe (nécessite graphviz)\\n\",\n",
        "    \"# C'est très utile pour vérifier que nos connexions sont correctes.\\n\",\n",
        "    \"try:\\n\",\n",
        "    \"    from IPython.display import Image, display\\n\",\n",
        "    \"    display(Image(app.get_graph().draw_png()))\\n\",\n",
        "    \"except ImportError:\\n\",\n",
        "    \"    print(\\\"Graphviz non installé. Impossible d'afficher le graphe.\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"Graphe compilé et prêt.\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"# --- Tests --- \\n\",\n",
        "    \"\\n\",\n",
        "    \"import time\\n\",\n",
        "    \"\\n\",\n",
        "    \"def run_test(question):\\n\",\n",
        "    \"    print(f\\\"\\\\n--- TEST AVEC LA QUESTION : '{question}' ---\\\")\\n\",\n",
        "    \"    inputs = {\\\"messages\\\": [HumanMessage(content=question)]}\\n\",\n",
        "    \"    final_state = app.invoke(inputs)\\n\",\n",
        "    \"    final_response = final_state['messages'][-1]\\n\",\n",
        "    \"    print(\\\"\\\\n--- RÉPONSE FINALE ---\\\")\\n\",\n",
        "    \"    print(final_response.content)\\n\",\n",
        "    \"    print(\\\"-------------------------\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Test 1: Salutation simple\\n\",\n",
        "    \"# run_test(\\\"Bonjour, comment ça va ?\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Test 2: Question précise\\n\",\n",
        "    \"# run_test(\\\"Qu'est-ce que le RAG auto-correctif ?\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Test 3: Question vague\\n\",\n",
        "    \"# run_test(\\\"Parle-moi des agents.\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Test 4: Question hors-sujet\\n\",\n",
        "    \"# run_test(\\\"Quelle est la meilleure recette de crêpes ?\\\")\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"markdown\",\n",
        "   \"metadata\": {},\n",
        "   \"source\": [\n",
        "    \"### Étape 7 : Fonction d'Interface pour Streamlit\\n\",\n",
        "    \"\\n\",\n",
        "    \"Pour que notre application Streamlit puisse utiliser la logique que nous venons de construire, nous devons créer une fonction simple qui sert de pont. \\n\",\n",
        "    \"\\n\",\n",
        "    \"Cette fonction, `run_agent`, prendra une chaîne de caractères (la question de l'utilisateur) en entrée et retournera la réponse finale de l'agent. Pour une meilleure expérience utilisateur dans Streamlit, nous allons la transformer en **générateur**. Elle produira la réponse morceau par morceau (`stream`), ce qui permettra d'afficher la réponse au fur et à mesure qu'elle est générée, donnant une impression de \\\"live typing\\\".\"\n",
        "   ]\n",
        "  },\n",
        "  {\n",
        "   \"cell_type\": \"code\",\n",
        "   \"execution_count\": None,\n",
        "   \"metadata\": {},\n",
        "   \"outputs\": [],\n",
        "   \"source\": [\n",
        "    \"import time\\n\",\n",
        "    \"\\n\",\n",
        "    \"def run_agent(user_query: str):\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    Exécute le graphe de l'agent pour une requête utilisateur donnée et streame la réponse.\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    Args:\\n\",\n",
        "    \"        user_query: La question posée par l'utilisateur.\\n\",\n",
        "    \"        \\n\",\n",
        "    \"    Yields:\\n\",\n",
        "    \"        str: Des morceaux de la réponse finale de l'agent.\\n\",\n",
        "    \"    \\\"\\\"\\\"\\n\",\n",
        "    \"    inputs = {\\\"messages\\\": [HumanMessage(content=user_query)]}\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Utiliser .stream() au lieu de .invoke() pour obtenir un générateur\\n\",\n",
        "    \"    # Cela nous permet de traiter les événements au fur et à mesure qu'ils se produisent dans le graphe.\\n\",\n",
        "    \"    full_response = \\\"\\\"\\n\",\n",
        "    \"    \\n\",\n",
        "    \"    # Le stream retourne les états intermédiaires du graphe.\\n\",\n",
        "    \"    # Nous nous intéressons au dernier message du dernier état.\\n\",\n",
        "    \"    for output in app.stream(inputs):\\n\",\n",
        "    \"        # La clé du dictionnaire correspond au nom du noeud qui vient de s'exécuter\\n\",\n",
        "    \"        for key, value in output.items():\\n\",\n",
        "    \"            if key == \\\"__end__\\\": # La fin du graphe\\n\",\n",
        "    \"                break\\n\",\n",
        "    \"            # Nous pouvons ajouter des logs ici pour voir la progression\\n\",\n",
        "    \"            # print(f\\\"Output from node '{key}': {value}\\\")\\n\",\n",
        "    \"            pass\\n\",\n",
        "    \"            \\n\",\n",
        "    \"    # Une fois le stream terminé, l'état final est dans la dernière valeur\\n\",\n",
        "    \"    final_state = value\\n\",\n",
        "    \"    if final_state and 'messages' in final_state and final_state['messages']:\\n\",\n",
        "    \"        final_response_message = final_state['messages'][-1]\\n\",\n",
        "    \"        full_response = final_response_message.content\\n\",\n",
        "    \"    else:\\n\",\n",
        "    \"        full_response = \\\"Désolé, je n'ai pas pu trouver de réponse.\\\"\\n\",\n",
        "    \"\\n\",\n",
        "    \"    # Simuler un streaming de la réponse finale pour l'affichage\\n\",\n",
        "    \"    for char in full_response:\\n\",\n",
        "    \"        yield char\\n\",\n",
        "    \"        time.sleep(0.01) # Petite pause pour un effet de frappe naturel\\n\",\n",
        "    \"\\n\",\n",
        "    \"print(\\\"La fonction 'run_agent' est prête à être appelée par Streamlit.\\\")\\n\",\n",
        "    \"\\n\",\n",
        "    \"# Exemple d'utilisation de la fonction run_agent\\n\",\n",
        "    \"# print(\\\"\\\\n--- TEST DE LA FONCTION STREAMLIT ---\\\")\\n\",\n",
        "    \"# query = \\\"Explique le concept de 'plan of thought' pour les agents IA.\\\"\\n\",\n",
        "    \"# response_generator = run_agent(query)\\n\",\n",
        "    \"# for chunk in response_generator:\\n\",\n",
        "    \"#     print(chunk, end=\\\"\\\", flush=True)\\n\",\n",
        "    \"# print()\"\n",
        "   ]\n",
        "  }\n",
        " ],\n",
        " \"metadata\": {\n",
        "  \"kernelspec\": {\n",
        "   \"display_name\": \"Python 3\",\n",
        "   \"language\": \"python\",\n",
        "   \"name\": \"python3\"\n",
        "  },\n",
        "  \"language_info\": {\n",
        "   \"codemirror_mode\": {\n",
        "    \"name\": \"ipython\",\n",
        "    \"version\": 3\n",
        "   },\n",
        "   \"file_extension\": \".py\",\n",
        "   \"mimetype\": \"text/x-python\",\n",
        "   \"name\": \"python\",\n",
        "   \"nbconvert_exporter\": \"python\",\n",
        "   \"pygments_lexer\": \"ipython3\",\n",
        "   \"version\": \"3.10.9\"\n",
        "  }\n",
        " },\n",
        " \"nbformat\": 4,\n",
        " \"nbformat_minor\": 4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e474a9d2",
        "outputId": "b9669ac5-eb34-492c-819b-d010e85022c9"
      },
      "source": [
        "%pip install python-dotenv langchain-groq langchain-google-genai langgraph langchain-community chromadb"
      ],
      "id": "e474a9d2",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (0.3.7)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.9)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (0.6.6)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.3.74)\n",
            "Requirement already satisfied: groq<1,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.31.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<0.7.0,>=0.6.18 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.6.18)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (2.1.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.6.4)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.2)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.14)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.9)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain-groq) (1.33)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.125->langchain-community) (0.24.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain-groq) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}